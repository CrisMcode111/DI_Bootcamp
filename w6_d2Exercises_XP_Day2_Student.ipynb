{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrisMcode111/DI_Bootcamp/blob/main/w6_d2Exercises_XP_Day2_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESaY5jKVlw4D"
      },
      "source": [
        "# Exercises XP: Day 2\n",
        "Follow the instructions. Where you see TODO, add your answer before running/continuing.\n"
      ],
      "id": "ESaY5jKVlw4D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD4TEVRSlw4F"
      },
      "source": [
        "## What You'll Learn\n",
        "- Deepen your understanding of core LLM concepts.\n",
        "- Apply theory to practical scenarios.\n",
        "- Develop critical thinking on LLM applications and ethics.\n",
        "- Compare/contrast transformer architectures and techniques.\n"
      ],
      "id": "wD4TEVRSlw4F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe3HxevBlw4H"
      },
      "source": [
        "## What You Will Create\n",
        "- Comparative tables (NLP paradigms and BERT variants)\n",
        "- Architecture/application write-ups\n",
        "- Pretraining benefits and ethical considerations\n",
        "- Analyses on attention and positional encoding\n",
        "- Model selection justifications across tasks\n",
        "- Notes on softmax temperature and its effects\n",
        "- Scenario-based answers applying learned concepts\n"
      ],
      "id": "Oe3HxevBlw4H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vUwNDmilw4H"
      },
      "source": [
        "## üåü Exercise 1: Traditional vs. Modern NLP: Comparative Analysis\n",
        "1) Complete the table (replace each TODO):\n",
        "\n",
        "| Aspect | Traditional NLP | Modern NLP |\n",
        "|---|---|---|\n",
        "| Feature Engineering | Manual ‚Äî based on human-designed rules and statistical features (bag-of-words, TF-IDF, n-grams) | Automatic ‚Äî neural networks learn representations directly from data (embeddings, contextual vectors) |\n",
        "| Word Representations | Sparse and independent (one-hot, TF-IDF) | Dense and contextual (Word2Vec, GloVe, BERT, GPT) |\n",
        "| Model Architectures | Statistical or classic ML models (Naive Bayes, SVM, Logistic Regression) | Deep learning architectures ‚Äî RNNs, LSTMs, Transformers |\n",
        "| Training Methodology | Fully supervised ‚Äî requires labeled data for each task | Pretraining on large unlabeled corpora + fine-tuning on specific tasks |\n",
        "| Key Examples | spaCy, NLTK, bag-of-words sentiment models | BERT, GPT-2/3, T5, LLaMA, ChatGPT |\n",
        "| Advantages | Simple, fast, easy to interpret, low computational cost|High performance, captures context, strong generalization |\n",
        "| Disadvantages | Limited accuracy, no deep contextual understanding, requires manual feature design | Complex, resource-intensive, less interpretable, harder to train|\n",
        "\n",
        "2) Discuss: How did the shift to modern NLP impact scalability and efficiency?\n",
        "\n",
        "- TODO: Write 5-7 sentences connecting representation learning, hardware parallelism, and transfer learning.\n",
        "The shift to modern NLP transformed scalability and efficiency through representation learning and transfer learning.\n",
        "Instead of manually crafting features, neural networks now learn language representations automatically, which scales easily across tasks and domains.\n",
        "With the rise of Transformer architectures, computation became highly parallelizable ‚Äî models can process entire sequences simultaneously, unlike RNNs that handled data step by step.\n",
        "This parallelism allows training on GPUs and TPUs, dramatically reducing training time for massive datasets.\n",
        "Pretrained models like BERT and GPT introduced transfer learning, enabling one model to be fine-tuned efficiently for many tasks with minimal data.\n",
        "As a result, NLP systems became both more scalable and more efficient, leveraging shared knowledge instead of starting from scratch for every problem.\n",
        "However, this progress also came with increased hardware demands and energy consumption, highlighting a new balance between performance and efficiency.\n"
      ],
      "id": "5vUwNDmilw4H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKh4AIILlw4I"
      },
      "source": [
        "## üåü Exercise 2: LLM Architecture and Application Scenarios\n",
        "For each, describe (a) core architectural differences, (b) a real application, (c) why it fits.\n",
        "\n",
        "### BERT\n",
        "- TODO: Bidirectional encoder-only; MLM (and historically NSP)\n",
        "- Application: TODO (e.g., classification/NER/Q&A)\n",
        "- Why: TODO\n",
        "\n",
        "### GPT\n",
        "- TODO: Decoder-only; autoregressive causal LM\n",
        "- Application: TODO (e.g., chat/generation/code)\n",
        "- Why: TODO\n",
        "\n",
        "### T5\n",
        "- TODO: Encoder-decoder; text-to-text framework\n",
        "- Application: TODO (e.g., summarization/translation)\n",
        "- Why: TODO\n"
      ],
      "id": "FKh4AIILlw4I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f765736"
      },
      "source": [
        "## üåü Exercise 2: LLM Architecture and Application Scenarios\n",
        "For each, describe (a) core architectural differences, (b) a real application, (c) why it fits.\n",
        "\n",
        "### BERT\n",
        "- Bidirectional encoder-only; pre-trained using Masked Language Modeling (MLM) and historically Next Sentence Prediction (NSP).\n",
        "- Application: Question Answering (e.g., searching for answers within a document).\n",
        "- Why: BERT's bidirectional nature allows it to understand the context of a word based on both the words that come before and after it in a sentence, which is crucial for accurately identifying the answer span in a question answering task. The encoder-only architecture is well-suited for tasks that require understanding the entire input sequence, rather than generating new text.\n",
        "\n",
        "### GPT\n",
        "- Decoder-only; autoregressive causal language model.\n",
        "- Application: Text generation (e.g., writing stories, articles, or code).\n",
        "- Why: GPT's decoder-only architecture and autoregressive nature make it excellent at generating sequential text, predicting the next token based on the preceding ones. This is ideal for creative writing, chatbots, and any task where the output is a continuation of the input.\n",
        "\n",
        "### T5\n",
        "- Encoder-decoder; pre-trained using a text-to-text framework, often involving denoising objectives.\n",
        "- Application: Machine Translation (e.g., translating text from one language to another).\n",
        "- Why: T5's encoder-decoder architecture is inherently suited for sequence-to-sequence tasks like translation, where the input is a sequence in one language and the output is a sequence in another. The text-to-text framework allows it to handle various translation tasks under a unified approach."
      ],
      "id": "1f765736"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh-bVJ6tlw4J"
      },
      "source": [
        "## üåü Exercise 3: Benefits and Ethics of Pre-training\n",
        "- Benefits (explain each in your words):\n",
        "  1) Improved generalization: Pre-training helps a model understand general language patterns before learning a specific task.\n",
        "Because it has already seen millions of examples, it can recognize grammar, meanings, and relationships between words.\n",
        "This makes it easier for the model to perform well on new or unseen data, even when it wasn‚Äôt trained directly on it.\n",
        "In short ‚Äî it ‚Äúlearns how to learn‚Äù and can adapt faster to different problems.\n",
        "  2) Less labeled data: Pre-training allows models to learn a lot from unlabeled text, like articles or conversations, before fine-tuning.\n",
        "When we later train them on a specific task (like spam detection or translation), they already ‚Äúknow‚Äù language patterns.\n",
        "This means we need much fewer labeled examples, saving time and effort because labeling data is expensive and slow.\n",
        "The model reuses its previous knowledge instead of starting from zero.\n",
        "  3) Faster fine-tuning: Faster fine-tuning:\n",
        "Because the model is already trained on a huge amount of text, it starts with a good understanding of language.\n",
        "When we fine-tune it for a specific task, it only needs to adjust slightly ‚Äî not learn everything from the beginning.\n",
        "This makes the training process much faster and requires less computing power.\n",
        "It‚Äôs like teaching someone who already speaks the language, instead of starting from zero.\n",
        "  4) Transfer learning: Transfer learning means taking a model that was already trained on one big, general task and reusing it for another, smaller task.\n",
        "The model ‚Äútransfers‚Äù its language knowledge ‚Äî grammar, meaning, and context ‚Äî to new problems like sentiment analysis or question answering.\n",
        "This saves a lot of time and data because we don‚Äôt need to train a new model from scratch every time.\n",
        "It helps achieve good results even when the new dataset is small.\n",
        "  5) Robustness: Pre-trained models are more stable and reliable because they‚Äôve already seen many types of text, topics, and writing styles.\n",
        "This helps them handle spelling mistakes, slang, or unusual phrasing better than models trained on small datasets.\n",
        "They don‚Äôt get confused as easily by noise or rare words.\n",
        "In short, pre-training makes models more resistant to errors and better at understanding real-world language.\n",
        "\n",
        "- Ethical concerns: TODO (bias, misinformation, misuse, privacy) Pre-trained models learn from huge amounts of text collected from the internet ‚Äî and that text often includes biases, stereotypes, and misinformation.\n",
        "As a result, the model can accidentally repeat or amplify unfair or false ideas.\n",
        "There is also a risk of misuse, like generating fake news or harmful content.\n",
        "Another problem is privacy, since training data may contain personal information that should not be exposed.\n",
        "That‚Äôs why developers must carefully check data sources and apply filters or ethical rules before using these models.\n",
        "- Mitigations: TODO (data curation, differential privacy, safety filters, RLHF, audits)\n",
        "To reduce ethical risks, developers use several safety methods.\n",
        "Data curation helps by cleaning and selecting high-quality, diverse, and unbiased text sources.\n",
        "Differential privacy protects personal data during training, so individual information cannot be traced.\n",
        "Safety filters and RLHF (Reinforcement Learning from Human Feedback) teach the model to avoid toxic, biased, or unsafe outputs.\n",
        "Regular audits and evaluations help detect problems early and keep the system transparent and responsible.\n",
        "Together, these steps make modern NLP models safer and more trustworthy."
      ],
      "id": "dh-bVJ6tlw4J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yZM0M7Xlw4K"
      },
      "source": [
        "## üåü Exercise 4: Transformer Architecture Deep Dive\n",
        "### Self-Attention & Multi-Head Attention\n",
        "- TODO: Describe the Q/K/V flow, softmax weighting, and value mixing. In a Transformer, each token is first converted into three vectors ‚Äî Query (Q), Key (K), and Value (V) ‚Äî through learned linear layers.\n",
        "The model compares every Query with all Keys to measure how strongly each word should pay attention to the others.\n",
        "These similarity scores are scaled and passed through a softmax function to turn them into attention weights (they sum to 1).\n",
        "The weights are then used to mix the Value vectors ‚Äî this gives a new representation for each word that includes information from its context.\n",
        "In Multi-Head Attention, several self-attention layers run in parallel, each focusing on different types of relationships (syntax, meaning, position, etc.).\n",
        "Their outputs are then concatenated and combined, giving the model a richer and more complete understanding of the sequence.\n",
        "- TODO: Explain why multiple heads help (subspace projections, diverse relations).Using multiple attention heads allows the model to look at the same sentence from different perspectives.\n",
        "Each head learns a different subspace projection of the input ‚Äî for example, one head may focus on grammar, another on meaning, and another on long-distance dependencies.\n",
        "By combining these diverse attention patterns, the model captures richer and more complex relationships between words.\n",
        "This makes the overall understanding more accurate and robust than using a single head.\n",
        "- Example sentence (different from lesson):‚ÄúThe cat sat on the mat because it was warm.‚Äù\n",
        "  - TODO: Provide a sentence and describe at least two different relations distinct heads might capture. One attention head might focus on the grammatical structure, linking ‚Äúcat‚Äù (subject) with ‚Äúsat‚Äù (verb).\n",
        "\n",
        "Another head could capture the coreference relationship, connecting ‚Äúit‚Äù with ‚Äúthe cat‚Äù to understand who ‚Äúit‚Äù refers to.\n",
        "This shows how different heads specialize ‚Äî one in syntax, another in meaning or reference ‚Äî to build a complete understanding of the sentence.\n",
        "\n",
        "### Pre-training Objectives\n",
        "- MLM vs CLM: TODO (compare objectives and typical use) MLM (Masked Language Modeling) ‚Äî used in models like BERT.\n",
        "It randomly hides some words in a sentence and trains the model to predict the missing words based on the context around them.\n",
        "This helps the model understand both left and right context, making it great for tasks like classification, QA, or sentence understanding.\n",
        "\n",
        "CLM (Causal Language Modeling) ‚Äî used in models like GPT.\n",
        "The model reads text from left to right and learns to predict the next word in a sequence.\n",
        "It captures strong generative abilities, making it ideal for text completion, dialogue, and creative generation tasks.\n",
        "- When to prefer MLM vs CLM: TODO Choose MLM when the goal is to understand text, not generate it ‚Äî for example in classification, sentiment analysis, or question answering.\n",
        "MLM helps the model see the full context (both sides) of each word, which improves comprehension and representation quality.\n",
        "\n",
        "Choose CLM when the goal is to generate text ‚Äî like chatbots, translation, story writing, or code generation.\n",
        "CLM trains the model to think ‚Äúforward,‚Äù predicting the next word naturally, which makes it fluent and coherent in generation tasks.\n",
        "- NSP: TODO (why early BERT used it and why many modern models avoid it)NSP (Next Sentence Prediction):\n",
        "Early BERT models used NSP to help the model understand relationships between sentences ‚Äî for example, whether one sentence logically follows another.\n",
        "This was useful for tasks like question answering or natural language inference, where sentence connections matter.\n",
        "\n",
        "However, later research showed that NSP did not significantly improve performance and sometimes even confused the model, because it focused too much on predicting sequence order instead of deeper meaning.\n",
        "Modern models often remove NSP and rely on better pre-training tasks (like Sentence Order Prediction ‚Äì SOP or larger context windows) that teach sentence relationships more effectively.\n",
        "\n",
        "### Transformer Model Selection\n",
        "- Sentiment on reviews: TODO (Encoder-only / Decoder-only / Encoder-Decoder) + justification Sentiment analysis focuses on understanding the text ‚Äî not generating new sentences.\n",
        "Encoder-only models read the entire input and build a deep contextual representation of its meaning, using both left and right context.\n",
        "This makes them ideal for classification tasks such as detecting positive or negative opinions in reviews.\n",
        "Decoder-only or encoder-decoder models are better suited for generation tasks, like translation or summarization, not for simple classification.\n",
        "- Conversational chatbot (creative responses): TODO + justification\n",
        "Model type: Decoder-only (like GPT-2, GPT-3, or ChatGPT)\n",
        "\n",
        "Justification:\n",
        "A chatbot needs to generate new text ‚Äî not just understand it.\n",
        "Decoder-only models are trained with a causal language modeling objective (predicting the next word), which makes them excellent at producing fluent, coherent, and creative responses.\n",
        "They naturally handle open-ended dialogue and can continue a conversation in context.\n",
        "Encoder-only models only analyze text, and encoder-decoder models are mainly used for structured generation (like translation or summarization), not free-form dialogue.\n",
        "- Technical document translation (EN‚ÜíES): TODO + justification Model type: Encoder‚ÄìDecoder (like T5, BART, or MarianMT)\n",
        "\n",
        "Justification:\n",
        "Translation requires both understanding the source text and generating the target text.\n",
        "The encoder reads and builds a full representation of the English sentence, while the decoder uses that information to produce the Spanish version word by word.\n",
        "This separation allows the model to preserve meaning and grammar across languages with high accuracy.\n",
        "Encoder-only models can understand but not generate, and decoder-only models can generate but lack direct cross-language alignment.\n",
        "\n",
        "### Transformers process all tokens in parallel, so they don‚Äôt naturally know the order of words in a sentence.\n",
        "Positional encoding adds information about each word‚Äôs position ‚Äî either absolute (its place in the sequence) or relative (its distance from other words).\n",
        "This helps the model understand word order, syntax, and meaning changes caused by position.\n",
        "For example, ‚Äúthe cat chased the dog‚Äù vs. ‚Äúthe dog chased the cat‚Äù have the same words but different meanings ‚Äî positional encoding helps the model tell them apart.\n",
        "- Example failure without positions: TODO If the model had no positional information, it would treat the sentences\n",
        " ‚ÄúThe cat chased the dog‚Äù and ‚ÄúThe dog chased the cat‚Äù\n",
        "as exactly the same, because they contain the same words.\n",
        "Without knowing the order, it can‚Äôt tell who is doing the action and who receives it.\n",
        "This would completely change the meaning and lead to wrong predictions or translations.\n"
      ],
      "id": "7yZM0M7Xlw4K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5745f47a"
      },
      "source": [
        "## üåü Exercise 5: BERT Variations: Choose Your Detective\n",
        "Assign the best fit and justify:\n",
        "- Scenario 1 (mobile, limited resources): DistilBERT - Smaller and faster than BERT, making it suitable for deployment on mobile devices or environments with limited computational resources.\n",
        "- Scenario 2 (legal docs, high accuracy): RoBERTa - Trained on a larger dataset with a modified training objective, leading to improved performance and accuracy, crucial for tasks requiring high precision like analyzing legal documents.\n",
        "- Scenario 3 (multilingual support): XLM-R - Pre-trained on a massive corpus of multilingual data, making it highly effective for cross-lingual transfer and tasks requiring understanding or processing text in multiple languages.\n",
        "- Scenario 4 (efficient pretraining with token replacement detection): ELECTRA - Uses a more efficient pre-training task (replaced token detection) that allows it to achieve better performance than BERT with less computational cost during pre-training.\n",
        "- Scenario 5 (efficient NLP, constrained environments): ALBERT - Uses parameter-reduction techniques like parameter sharing and factorization to significantly reduce the number of parameters, making it more memory-efficient and faster, ideal for constrained environments.\n",
        "\n",
        "Create/completed table:\n",
        "\n",
        "| Model | Training differences | Size/Efficiency | Innovations | Ideal use cases |\n",
        "|---|---|---|---|---|\n",
        "| RoBERTa | Trained longer on more data, dynamic masking, no NSP | Larger than BERT, improved performance | Dynamic masking, larger batch sizes | High-accuracy tasks, general NLP tasks |\n",
        "| ALBERT | Parameter sharing across layers, factorized embedding parameterization | Significantly smaller and faster than BERT | Parameter reduction techniques | Resource-constrained environments, fine-tuning tasks |\n",
        "| DistilBERT | Knowledge distillation from BERT | Smaller and faster than BERT (reduced layers) | Knowledge distillation | Mobile/edge devices, latency-sensitive applications |\n",
        "| ELECTRA | Replaced Token Detection (RTD) as pre-training task | More efficient pre-training than BERT | Discriminator-based pre-training | Efficient pre-training, good performance with smaller models |\n",
        "| XLM-R | Trained on 100 languages using Masked Language Modeling | Large, multilingual | Cross-lingual pre-training | Multilingual tasks, cross-lingual transfer |"
      ],
      "id": "5745f47a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REmltGLglw4L"
      },
      "source": [
        "## üåü Exercise 6: Softmax Temperature: The Randomness Regulator\n",
        "### 1) Temperature Scenarios\n",
        "- T=0.2: TODO The temperature is very low, so the model becomes confident and conservative.\n",
        "It mostly chooses the most probable word, producing short, predictable, and repetitive text.\n",
        "Good for factual or precise answers, but creativity drops.\n",
        "- T=1.5: TODO This is the default, balanced temperature.\n",
        "The model mixes accuracy and diversity, producing coherent yet natural responses.\n",
        "It‚Äôs ideal for most conversational and creative tasks.\n",
        "- T=1.0: TODO A high temperature makes the model more random and creative.\n",
        "It explores unusual or unexpected words and ideas, which can make text more interesting ‚Äî but sometimes less logical or coherent.\n",
        "\n",
        "### 2) Application Design\n",
        "- Bedtime stories (creativity vs coherence): TODO (temperature strategy and why)For bedtime stories, we want the text to be imaginative but still easy to follow.\n",
        "A good strategy is to use a medium-to-high temperature, around 0.8‚Äì1.2.\n",
        "This gives the model enough creativity to invent magical scenes, new characters, and playful twists ‚Äî without becoming too random or confusing.\n",
        "If the temperature is too low, the story will sound repetitive and dull; if it‚Äôs too high, it might lose structure and logic.\n",
        "- Financial report summaries (accuracy/reliability): TODO (temperature/decoding strategy and why)Financial report summaries (accuracy/reliability):\n",
        "For financial texts, precision and factual consistency are more important than creativity.\n",
        "Use a low temperature, around 0.2‚Äì0.4, so the model stays focused on the most probable and reliable words.\n",
        "This reduces randomness and prevents the generation of false or exaggerated information.\n",
        "You can also combine it with a greedy or beam search decoding strategy to ensure the output is clear, stable, and data-driven.\n",
        "\n",
        "### 3) Temperature & Bias\n",
        "- TODO: How does temperature influence bias surfacing or dampening? Give a realistic example.Temperature affects not only creativity but also how strongly a model‚Äôs biases appear in its responses.\n",
        "At a low temperature, the model mostly chooses its most confident (high-probability) answers ‚Äî which can reinforce existing biases, since those are often learned as dominant patterns from training data.\n",
        "At a higher temperature, the model samples more diverse words, which can dilute or soften those biases but might also introduce random or inconsistent phrasing.\n",
        "\n",
        "Example:\n",
        "If asked to describe a ‚Äúleader,‚Äù a model with low temperature might always say ‚Äúa man in a suit,‚Äù repeating a bias seen in data.\n",
        "At a higher temperature, it could produce varied answers like ‚Äúa woman inspiring her team‚Äù or ‚Äúa young activist organizing change.‚Äù\n"
      ],
      "id": "REmltGLglw4L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysyi_O4ylw4L"
      },
      "source": [
        "### (Optional) Quick Generation Demo Across Temperatures\n",
        "Note: Requires `transformers` and model download; skip if offline.\n"
      ],
      "id": "Ysyi_O4ylw4L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh4yVFodlw4M"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"  # TODO: choose a causal LM\n",
        "prompt = \"Artificial intelligence will\"  # TODO: write your own prompt\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=AutoModelForCausalLM.from_pretrained(model_name),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "for T in [0.2, 1.0, 1.5]:\n",
        "    out = pipe(prompt, max_new_tokens=40, temperature=T, do_sample=True)\n",
        "    print(\"--- Temperature:\", T)\n",
        "    print(out[0][\"generated_text\"])  # Inspect how style changes with T\n"
      ],
      "id": "Bh4yVFodlw4M"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}