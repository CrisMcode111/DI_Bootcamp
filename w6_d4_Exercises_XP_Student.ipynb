{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrisMcode111/DI_Bootcamp/blob/main/w6_d4_Exercises_XP_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1507c2f",
      "metadata": {
        "id": "c1507c2f"
      },
      "source": [
        "# Exercises XP: LoRA Implementation Lab\n",
        "Replace each `TODO` before running the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db322f5",
      "metadata": {
        "id": "2db322f5"
      },
      "source": [
        "## What you'll learn\n",
        "\n",
        "- The fundamentals of LoRA (Low-Rank Adaptation) and why it helps churn out efficient fine-tunes.\n",
        "- How to implement LoRA matrices `A` and `B`, plus how to wrap existing `nn.Linear` layers.\n",
        "- Differences between standard linear layers, LoRA-enhanced layers, and merged-weight alternatives.\n",
        "- How to freeze base parameters so that only the LoRA adapters receive updates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562dac20",
      "metadata": {
        "id": "562dac20"
      },
      "source": [
        "## What you will create\n",
        "\n",
        "- A reusable `LoRALayer` module and two linear wrappers (`LinearWithLoRA`, `LinearWithLoRAMerged`).\n",
        "- A 3-layer MLP that can be swapped between standard and LoRA-enhanced variants.\n",
        "- A minimal MNIST training loop plus accuracy helpers to compare frozen vs. fully-trainable adapters.\n",
        "- A workflow to freeze baseline weights and fine-tune only the LoRA layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d60a52d",
      "metadata": {
        "id": "1d60a52d"
      },
      "source": [
        "> **Learning point**  \n",
        "> Keep the student and teacher notebooks open side by side. Follow the numbered exercises, run setup only once, and watch tensor shapes as you add LoRA adapters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a488f974",
      "metadata": {
        "id": "a488f974"
      },
      "source": [
        "# Part 0: Environment Setup\n",
        "\n",
        "Install the CPU-friendly PyTorch stack plus torchvision for MNIST. Reuse caches across reruns to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd69cab",
      "metadata": {
        "id": "9cd69cab"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3d6c10",
      "metadata": {
        "id": "bd3d6c10",
        "outputId": "1c4722f5-11af-4498-b851-b832bb17d4b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "BASE_SEED = 123\n",
        "torch.manual_seed(BASE_SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ed4830",
      "metadata": {
        "id": "68ed4830"
      },
      "source": [
        "# Exercise 1: Implement `LoRALayer`\n",
        "\n",
        "Create the low-rank matrices `A` and `B`, scale them with `alpha`, and test the module on a toy tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbf9dee",
      "metadata": {
        "id": "1dbf9dee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3bb95e-8b30-4229-8cdf-7fab996b79fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x:\n",
            " tensor([[-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010],\n",
            "        [ 0.9624,  0.2492, -0.4845, -2.0929, -0.8199, -0.4210, -0.9620,  1.2825],\n",
            "        [-0.3430, -0.6821, -0.9887, -1.7018, -0.7498, -1.1285,  0.4135,  0.2892]])\n",
            "\n",
            "Layer:\n",
            " LoRALayer()\n",
            "\n",
            "LoRA update output (Δy):\n",
            " tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]], grad_fn=<MulBackward0>)\n",
            "Output shape: torch.Size([3, 5])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "        self.rank = rank # Added to store rank as an instance attribute\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Low-rank update Δy = x @ A @ B, scaled by α/r (LoRA convention)\n",
        "        scale = self.alpha / self.rank\n",
        "        return (x @ self.A @ self.B) * scale\n",
        "\n",
        "# Hyperparameters for the sandbox test\n",
        "random_seed = 123\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "in_dim = 8\n",
        "out_dim = 5\n",
        "rank = 2\n",
        "alpha = 4.0\n",
        "\n",
        "layer = LoRALayer(in_dim, out_dim, rank, alpha)\n",
        "\n",
        "batch = 3\n",
        "x = torch.randn(batch, in_dim)\n",
        "\n",
        "print(\"Input x:\\n\", x)\n",
        "print(\"\\nLayer:\\n\", layer)\n",
        "y = layer(x)\n",
        "print(\"\\nLoRA update output (Δy):\\n\", y)\n",
        "print(\"Output shape:\", y.shape)  # should be (batch, out_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b126ad",
      "metadata": {
        "id": "01b126ad"
      },
      "source": [
        "# Exercise 2: Wrap `nn.Linear` with LoRA\n",
        "\n",
        "Combine a frozen linear projection plus a trainable `LoRALayer`. Confirm the adapter outputs add on top of the base logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303deffa",
      "metadata": {
        "id": "303deffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbd8835-d248-4c7d-ecdd-4089acd3ca09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearWithLoRA output: tensor([[-0.1224,  0.2353,  0.2788, -0.9573,  0.7254],\n",
            "        [ 0.6466,  0.4186,  0.2505,  0.9226,  0.0839],\n",
            "        [ 0.1024,  0.6865,  0.7498,  0.1414, -0.4729]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features,\n",
        "            linear.out_features,\n",
        "            rank,\n",
        "            alpha,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "base_linear = nn.Linear(in_dim, out_dim)\n",
        "layer_lora_1 = LinearWithLoRA(base_linear, rank, alpha)\n",
        "print(\"LinearWithLoRA output:\", layer_lora_1(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708b0585",
      "metadata": {
        "id": "708b0585"
      },
      "source": [
        "# Exercise 3: Swap a simple network layer with LoRA\n",
        "\n",
        "Start from a single-layer perceptron, then replace its linear block with `LinearWithLoRA`. The outputs should match before training because the LoRA adapters start at zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0e54d5",
      "metadata": {
        "id": "8c0e54d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007d6d09-3218-4e87-d2c8-9fd5bead1dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs match before training? True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SingleLayerNet(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "num_features = 8\n",
        "num_classes  = 5\n",
        "sample_input = torch.randn(4, num_features)\n",
        "\n",
        "single_net = SingleLayerNet(num_features=num_features, num_classes=num_classes)\n",
        "\n",
        "with torch.no_grad():\n",
        "    baseline_output = single_net(sample_input)\n",
        "\n",
        "single_net.layer = LinearWithLoRA(single_net.layer, rank=rank, alpha=alpha)\n",
        "\n",
        "with torch.no_grad():\n",
        "    lora_output = single_net(sample_input)\n",
        "\n",
        "print(\"Outputs match before training?\", torch.allclose(baseline_output, lora_output, atol=1e-6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b71473",
      "metadata": {
        "id": "13b71473"
      },
      "source": [
        "# Exercise 4: Merged-weight LoRA layer\n",
        "\n",
        "Fuse the LoRA matrices with the frozen weights to create a drop-in linear layer that behaves exactly like `LinearWithLoRA`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7cccf8",
      "metadata": {
        "id": "5e7cccf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cae0153-0bf6-4b5d-cc1d-d92f1f60659e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged LoRA output: tensor([[-0.1224,  0.2353,  0.2788, -0.9573,  0.7254],\n",
            "        [ 0.6466,  0.4186,  0.2505,  0.9226,  0.0839],\n",
            "        [ 0.1024,  0.6865,  0.7498,  0.1414, -0.4729]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinearWithLoRAMerged(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features,\n",
        "            linear.out_features,\n",
        "            rank,\n",
        "            alpha,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora_matrix = self.lora.A @ self.lora.B\n",
        "        scale = self.lora.alpha / self.lora.rank\n",
        "        combined_weight = self.linear.weight + scale * lora_matrix.T\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n",
        "\n",
        "\n",
        "layer_lora_2 = LinearWithLoRAMerged(base_linear, rank, alpha)\n",
        "print(\"Merged LoRA output:\", layer_lora_2(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c529738c",
      "metadata": {
        "id": "c529738c"
      },
      "source": [
        "# Exercise 5: Build an MLP and prepare MNIST\n",
        "\n",
        "Stack three linear layers with ReLU activations, then set up the MNIST loaders plus optimizer/state for pretraining."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(num_features, num_hidden_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_hidden_1, num_hidden_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_hidden_2, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)   # <-- asta lipsea (flatten: (B,1,28,28)->(B,784))\n",
        "        x = self.layers(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "c5QZo8JgVvYl"
      },
      "id": "c5QZo8JgVvYl",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "620ea879",
      "metadata": {
        "id": "620ea879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501acd93-68f4-4cb2-82d9-4dcf5988be15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "MultilayerPerceptron(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Architecture\n",
        "num_features = 784\n",
        "num_hidden_1 = 256\n",
        "num_hidden_2 = 128\n",
        "num_classes = 10\n",
        "\n",
        "# Settings\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 5\n",
        "\n",
        "model = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_pretrained = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.to(DEVICE)\n",
        "print(DEVICE)\n",
        "print(model)\n",
        "print(optimizer_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f3a6b5",
      "metadata": {
        "id": "a7f3a6b5"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PKtCOT5Ukdy",
        "outputId": "97bda8e8-b9ca-4be5-efe6-e26a4c9fd53a"
      },
      "id": "4PKtCOT5Ukdy",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f352b30",
      "metadata": {
        "id": "5f352b30"
      },
      "source": [
        "## Define evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.view(features.size(0), -1).to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum().item()\n",
        "    return correct_pred / num_examples\n"
      ],
      "metadata": {
        "id": "spHS2xUPXX26"
      },
      "id": "spHS2xUPXX26",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "202bce1c",
      "metadata": {
        "id": "202bce1c"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def train(num_epochs, model, optimizer, train_loader, device):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "            features = features.view(features.size(0), -1).to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            loss = criterion(logits, targets)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not batch_idx % 400:\n",
        "                print('Epoch: %03d/%03d|Batch %03d/%03d| Loss: %.4f' %\n",
        "                      (epoch+1, num_epochs, batch_idx, len(train_loader), loss))\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            print('Epoch: %03d/%03d training accuracy: %.2f%%' %\n",
        "                  (epoch+1, num_epochs, compute_accuracy(model, train_loader, device)*100))\n",
        "\n",
        "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n"
      ],
      "metadata": {
        "id": "k_juFsYwVG4A"
      },
      "id": "k_juFsYwVG4A",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "18d8aeae",
      "metadata": {
        "id": "18d8aeae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31664220-315d-482d-d3b4-ab91fd872665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-755968686.py:20: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  print('Epoch: %03d/%03d|Batch %03d/%03d| Loss: %.4f' %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/005|Batch 000/938| Loss: 2.3046\n",
            "Epoch: 001/005|Batch 400/938| Loss: 0.3282\n",
            "Epoch: 001/005|Batch 800/938| Loss: 0.1142\n",
            "Epoch: 001/005 training accuracy: 96.60%\n",
            "Epoch: 002/005|Batch 000/938| Loss: 0.1085\n",
            "Epoch: 002/005|Batch 400/938| Loss: 0.0925\n",
            "Epoch: 002/005|Batch 800/938| Loss: 0.1300\n",
            "Epoch: 002/005 training accuracy: 97.84%\n",
            "Epoch: 003/005|Batch 000/938| Loss: 0.0439\n",
            "Epoch: 003/005|Batch 400/938| Loss: 0.0800\n",
            "Epoch: 003/005|Batch 800/938| Loss: 0.0912\n",
            "Epoch: 003/005 training accuracy: 98.16%\n",
            "Epoch: 004/005|Batch 000/938| Loss: 0.1841\n",
            "Epoch: 004/005|Batch 400/938| Loss: 0.0838\n",
            "Epoch: 004/005|Batch 800/938| Loss: 0.0130\n",
            "Epoch: 004/005 training accuracy: 98.78%\n",
            "Epoch: 005/005|Batch 000/938| Loss: 0.0452\n",
            "Epoch: 005/005|Batch 400/938| Loss: 0.1017\n",
            "Epoch: 005/005|Batch 800/938| Loss: 0.0533\n",
            "Epoch: 005/005 training accuracy: 99.00%\n",
            "Time elapsed: 1.95 min\n",
            "Total Training Time: 1.95 min\n",
            "Test accuracy: 0.98%\n"
          ]
        }
      ],
      "source": [
        "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
        "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa596a9",
      "metadata": {
        "id": "2fa596a9"
      },
      "source": [
        "# Replacing Linear with LoRA Layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LinearWithLoRAMerged(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features,\n",
        "            linear.out_features,\n",
        "            rank,\n",
        "            alpha,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora_matrix = self.lora.A @ self.lora.B\n",
        "        scale = self.lora.alpha / self.lora.rank\n",
        "        combined_weight = self.linear.weight + scale * lora_matrix.T\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n"
      ],
      "metadata": {
        "id": "4GdI2wptZTti"
      },
      "id": "4GdI2wptZTti",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank, dtype=torch.float32))\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x @ self.A @ self.B) * (self.alpha / self.rank)\n"
      ],
      "metadata": {
        "id": "S3MjTU_0ZqQ2"
      },
      "id": "S3MjTU_0ZqQ2",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "model_lora = copy.deepcopy(model)\n",
        "\n",
        "model_lora.layers[0] = LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
        "model_lora.layers[2] = LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
        "model_lora.layers[4] = LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
        "\n",
        "model_lora.to(DEVICE)\n",
        "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "print(model_lora)\n",
        "\n",
        "print(f'Test accuracy orig model:{compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model:{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usjd0wjqY-Of",
        "outputId": "42156458-f27f-4c98-e18a-87a1f4b5ae0b"
      },
      "id": "usjd0wjqY-Of",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultilayerPerceptron(\n",
            "  (layers): Sequential(\n",
            "    (0): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=784, out_features=256, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (1): ReLU()\n",
            "    (2): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (3): ReLU()\n",
            "    (4): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=128, out_features=10, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test accuracy orig model:0.98%\n",
            "Test accuracy LoRA model:0.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eabe0b42",
      "metadata": {
        "id": "eabe0b42"
      },
      "source": [
        "## Freezing the Original Linear Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1d0f187b",
      "metadata": {
        "id": "1d0f187b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed648e78-7598-4d2d-ef57-fdd2c050c38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.linear.weight:False\n",
            "layers.0.linear.bias:False\n",
            "layers.0.lora.A:True\n",
            "layers.0.lora.B:True\n",
            "layers.2.linear.weight:False\n",
            "layers.2.linear.bias:False\n",
            "layers.2.lora.A:True\n",
            "layers.2.lora.B:True\n",
            "layers.4.linear.weight:False\n",
            "layers.4.linear.bias:False\n",
            "layers.4.lora.A:True\n",
            "layers.4.lora.B:True\n"
          ]
        }
      ],
      "source": [
        "def freeze_linear_layers(model):\n",
        "    for child in model.children():\n",
        "        if isinstance(child, nn.Linear):\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            freeze_linear_layers(child)\n",
        "\n",
        "freeze_linear_layers(model_lora)\n",
        "for name, param in model_lora.named_parameters():\n",
        "    print(f'{name}:{param.requires_grad}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6ef61a59",
      "metadata": {
        "id": "6ef61a59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa63036-c737-405c-9362-cf8ba6bb6426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/005|Batch 000/938| Loss: 0.0116\n",
            "Epoch: 001/005|Batch 400/938| Loss: 0.0158\n",
            "Epoch: 001/005|Batch 800/938| Loss: 0.0367\n",
            "Epoch: 001/005 training accuracy: 99.51%\n",
            "Epoch: 002/005|Batch 000/938| Loss: 0.0020\n",
            "Epoch: 002/005|Batch 400/938| Loss: 0.0158\n",
            "Epoch: 002/005|Batch 800/938| Loss: 0.0095\n",
            "Epoch: 002/005 training accuracy: 99.62%\n",
            "Epoch: 003/005|Batch 000/938| Loss: 0.0403\n",
            "Epoch: 003/005|Batch 400/938| Loss: 0.0034\n",
            "Epoch: 003/005|Batch 800/938| Loss: 0.0053\n",
            "Epoch: 003/005 training accuracy: 99.66%\n",
            "Epoch: 004/005|Batch 000/938| Loss: 0.0050\n",
            "Epoch: 004/005|Batch 400/938| Loss: 0.0063\n",
            "Epoch: 004/005|Batch 800/938| Loss: 0.0278\n",
            "Epoch: 004/005 training accuracy: 99.65%\n",
            "Epoch: 005/005|Batch 000/938| Loss: 0.0091\n",
            "Epoch: 005/005|Batch 400/938| Loss: 0.0086\n",
            "Epoch: 005/005|Batch 800/938| Loss: 0.0017\n",
            "Epoch: 005/005 training accuracy: 99.69%\n",
            "Time elapsed: 1.98 min\n",
            "Total Training Time: 1.98 min\n",
            "Test accuracy LoRA finetune: 0.98%\n",
            "Test accuracy orig model:0.98%\n",
            "Test accuracy LoRA model:0.98%\n"
          ]
        }
      ],
      "source": [
        "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
        "print(f'Test accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "print(f'Test accuracy orig model:{compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model:{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8AcHZm0cW0j"
      },
      "id": "E8AcHZm0cW0j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}