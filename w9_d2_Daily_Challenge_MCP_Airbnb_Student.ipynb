{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrisMcode111/DI_Bootcamp/blob/main/w9_d2_Daily_Challenge_MCP_Airbnb_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06f7a68",
      "metadata": {
        "id": "e06f7a68"
      },
      "source": [
        "# Student Notebook - MCP + Airbnb (Colab)\n",
        "\n",
        "Reference notebook: local notes MCP + Airbnb MCP + optional real LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da2a006",
      "metadata": {
        "id": "0da2a006"
      },
      "source": [
        "## Install\n",
        "Run once. npm only needed for the real Airbnb server."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup Notes\n",
        "\n",
        "The following cells install and configure all dependencies required for the MCP client/server workflow:\n",
        "- `mcp`, `nest_asyncio`, and `requests` for the local MCP client execution inside Colab.\n",
        "- `azure-ai-inference` for optional LLM integration.\n",
        "- The Airbnb MCP Server (`@openbnb/mcp-server-airbnb`) is installed globally via npm, enabling a real server backend when available.\n",
        "\n",
        "Because Google Colab does not provide traditional TTY file descriptors, the `OutStream.fileno()` method is patched so the MCP server spawned over STDIO can communicate correctly with the client. This workaround is required only in notebook environments.\n",
        "\n",
        "We also load environment variables:\n",
        "- `MCP_HTTP_TOKEN` used for server authentication.\n",
        "- `GITHUB_TOKEN` needed when enabling a real LLM backend.\n",
        "\n",
        "Up to this point, all setup commands executed successfully, meaning the environment is ready for running the MCP client and interacting with tools.\n"
      ],
      "metadata": {
        "id": "qDyhfd2COatc"
      },
      "id": "qDyhfd2COatc"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "35a5a9b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35a5a9b2",
        "outputId": "9abab9c2-a61f-4ce4-8d7a-0c620d1d1464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-inference\n",
            "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-inference)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core>=1.30.0 (from azure-ai-inference)\n",
            "  Downloading azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (4.15.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.11.12)\n",
            "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.36.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-core, azure-ai-inference\n",
            "Successfully installed azure-ai-inference-1.0.0b9 azure-core-1.36.0 isodate-0.7.2\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m node-domexception@1.0.0: Use your platform's native DOMException instead\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "added 117 packages in 12s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K42 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install -q mcp nest_asyncio requests\n",
        "!pip install azure-ai-inference\n",
        "\n",
        "# Optional: real Airbnb server\n",
        "!npm install -g @openbnb/mcp-server-airbnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "rQj-kTd2OX7_",
      "metadata": {
        "id": "rQj-kTd2OX7_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from ipykernel.iostream import OutStream\n",
        "\n",
        "def _patched_fileno(self):\n",
        "    # stdout → 1, stderr → 2\n",
        "    if self is sys.stderr:\n",
        "        return 2\n",
        "    return 1\n",
        "\n",
        "# Patch the class for all OutStream instances\n",
        "OutStream.fileno = _patched_fileno\n",
        "\n",
        "# And patch the current instances explicitly\n",
        "sys.stdout.fileno = lambda: 1\n",
        "sys.stderr.fileno = lambda: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aabe4925",
      "metadata": {
        "id": "aabe4925"
      },
      "source": [
        "## Config\n",
        "Flip toggles as needed. Keep defaults for stubbed run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0ee35bb4",
      "metadata": {
        "id": "0ee35bb4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "MCP_HTTP_TOKEN = os.getenv(\"MCP_HTTP_TOKEN\", \"devtoken123\")\n",
        "USE_REAL_AIRBNB = True  # True if npm server available\n",
        "USE_REAL_LLM = True     # True if GITHUB_TOKEN set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fmugjy9NHMe",
      "metadata": {
        "id": "0fmugjy9NHMe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "BASE_ENV = os.environ.copy()\n",
        "BASE_ENV[\"MCP_HTTP_TOKEN\"] = MCP_HTTP_TOKEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "26FpWA11P4s7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26FpWA11P4s7",
        "outputId": "db0abb4e-62f1-4ee8-b829-9161fd71a6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GITHUB_TOKEN visible to Python: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata  # Colab secrets API\n",
        "\n",
        "# If your secret is saved under the key \"GITHUB_TOKEN\" in Colab:\n",
        "os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# If you used a different key name in the secrets UI, e.g. \"github_token\":\n",
        "# os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"github_token\")\n",
        "\n",
        "print(\"GITHUB_TOKEN visible to Python:\", bool(os.getenv(\"GITHUB_TOKEN\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f40d6c",
      "metadata": {
        "id": "88f40d6c"
      },
      "source": [
        "## Local notes MCP server"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_SERVER = Path(\"local_notes_server.py\")\n",
        "LOCAL_SERVER.write_text(\n",
        "'''from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "notes = []\n",
        "\n",
        "# Construct FastMCP server instance\n",
        "mcp = FastMCP(\"local-notes-server\")\n",
        "\n",
        "# Register add_note as a tool\n",
        "@mcp.tool()\n",
        "def add_note(text: str) -> str:\n",
        "    \"Add a note to the in-memory list.\"\n",
        "    notes.append(text)\n",
        "    return f\"Saved note #{len(notes)}: {text}\"\n",
        "\n",
        "# Register list_notes as a tool\n",
        "@mcp.tool()\n",
        "def list_notes() -> str:\n",
        "    \"List saved notes.\"\n",
        "    if not notes:\n",
        "        return \"No notes yet\"\n",
        "    return \"\\\\n\".join(f\"{i+1}. {n}\" for i, n in enumerate(notes))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run()\n",
        "''',\n",
        "    encoding=\"utf-8\",\n",
        ")\n",
        "print(\"wrote\", LOCAL_SERVER)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDcOsoxeOqqe",
        "outputId": "9f4b0074-20e4-421a-f32a-5998715b155d"
      },
      "id": "gDcOsoxeOqqe",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrote local_notes_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8871e2",
      "metadata": {
        "id": "1b8871e2"
      },
      "source": [
        "## Client helpers (convert tools, stub planner, optional real LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "32319757",
      "metadata": {
        "id": "32319757"
      },
      "outputs": [],
      "source": [
        "\n",
        "import asyncio\n",
        "import json\n",
        "import nest_asyncio\n",
        "from typing import Any, Dict, List\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def convert_tool(tool, prefix: str):\n",
        "    # Azure requires ^[a-zA-Z0-9_\\.-]+$, so no slashes\n",
        "    fn_name = f\"{prefix}__{tool.name}\"\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": fn_name,\n",
        "            \"description\": tool.description or \"mcp tool\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": tool.inputSchema.get(\"properties\", {}),\n",
        "                \"required\": tool.inputSchema.get(\"required\", []),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def call_llm(prompt: str, functions: List[Dict[str, Any]], use_real: bool = True):\n",
        "    import os\n",
        "    import json\n",
        "    from azure.ai.inference import ChatCompletionsClient\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "    token = os.getenv(\"GITHUB_TOKEN\")\n",
        "    if not token:\n",
        "        raise RuntimeError(\"Set GITHUB_TOKEN or use stub planner.\")\n",
        "\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=\"https://models.inference.ai.azure.com\",\n",
        "        credential=AzureKeyCredential(token)\n",
        "    )\n",
        "\n",
        "    # Azure Chat Completion (OpenAI-compatible)\n",
        "    resp = client.complete(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        tools=functions,\n",
        "        tool_choice=\"auto\"\n",
        "    )\n",
        "\n",
        "    calls = []\n",
        "    msg = resp.choices[0].message\n",
        "\n",
        "    for tc in msg.tool_calls or []:\n",
        "        args = tc.function.arguments\n",
        "        args_json = json.loads(args) if isinstance(args, str) else args\n",
        "        calls.append({\n",
        "            \"name\": tc.function.name,\n",
        "            \"args\": args_json\n",
        "        })\n",
        "\n",
        "    return calls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "sGLsu8L0Tg0f",
      "metadata": {
        "id": "sGLsu8L0Tg0f"
      },
      "outputs": [],
      "source": [
        "def answer_with_llm(\n",
        "    user_prompt: str,\n",
        "    tool_calls: List[Dict[str, Any]],\n",
        "    tool_results: List[Dict[str, Any]],\n",
        "    use_real: bool = True,\n",
        ") -> str:\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    # MINIMAL FIX: shrink tool_results before sending to gpt-4o\n",
        "    small_results = []\n",
        "    for r in tool_results:\n",
        "        content = r.get(\"content\", [])\n",
        "        short_content = []\n",
        "        if content:\n",
        "            first = content[0]\n",
        "            if isinstance(first, str) and len(first) > 4000:\n",
        "                first = first[:4000] + \"...(truncated)...\"\n",
        "            short_content = [first]\n",
        "        small_results.append(\n",
        "            {\n",
        "                \"name\": r.get(\"name\"),\n",
        "                \"args\": r.get(\"args\", {}),\n",
        "                \"content\": short_content,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "    from azure.ai.inference import ChatCompletionsClient\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "    token = os.getenv(\"GITHUB_TOKEN\")\n",
        "    if not token:\n",
        "        raise RuntimeError(\"Set GITHUB_TOKEN or use_real=False in answer_with_llm.\")\n",
        "\n",
        "    client = ChatCompletionsClient(\n",
        "        \"https://models.inference.ai.azure.com\",\n",
        "        AzureKeyCredential(token),\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"user_question\": user_prompt,\n",
        "        \"tool_calls\": tool_calls,\n",
        "        # use the shrunk version here\n",
        "        \"tool_results\": small_results,\n",
        "    }\n",
        "\n",
        "    resp = client.complete(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You answer the user's question using the given tool outputs.\\n\"\n",
        "                    \"JSON contains user_question, tool_calls, and tool_results (already truncated).\\n\"\n",
        "                    \"1. Answer clearly in markdown.\\n\"\n",
        "                    \"2. At the end, add:\\n\"\n",
        "                    \"## Tools used\\n\"\n",
        "                    \"- One bullet per distinct tool name.\\n\"\n",
        "                ),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": json.dumps(payload, ensure_ascii=False),\n",
        "            },\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=600,\n",
        "    )\n",
        "\n",
        "    msg = resp.choices[0].message\n",
        "    parts = getattr(msg, \"content\", None)\n",
        "    if isinstance(parts, list):\n",
        "        texts = []\n",
        "        for p in parts:\n",
        "            text = getattr(p, \"text\", None) or getattr(p, \"content\", None)\n",
        "            if isinstance(text, str):\n",
        "                texts.append(text)\n",
        "        if texts:\n",
        "            return \"\".join(texts)\n",
        "\n",
        "    return str(msg.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95b04e1",
      "metadata": {
        "id": "d95b04e1"
      },
      "source": [
        "## Orchestrate (connect both servers and execute tool_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "75502252",
      "metadata": {
        "id": "75502252"
      },
      "outputs": [],
      "source": [
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "async def orchestrate(prompt: str):\n",
        "    local_params = StdioServerParameters(\n",
        "        command=\"mcp\",\n",
        "        args=[\"run\", str(LOCAL_SERVER)],\n",
        "        env=BASE_ENV,  # <- use merged env\n",
        "    )\n",
        "\n",
        "    if USE_REAL_AIRBNB:\n",
        "        airbnb_params = StdioServerParameters(\n",
        "            command=\"npx\",\n",
        "            args=[\"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"],\n",
        "            env=BASE_ENV,\n",
        "        )\n",
        "\n",
        "\n",
        "    async with stdio_client(local_params) as (lr, lw):\n",
        "        async with ClientSession(lr, lw) as local_sess:\n",
        "            await local_sess.initialize()\n",
        "            local_tools = await local_sess.list_tools()\n",
        "\n",
        "            async with stdio_client(airbnb_params) as (ar, aw):\n",
        "                async with ClientSession(ar, aw) as airbnb_sess:\n",
        "                    await airbnb_sess.initialize()\n",
        "                    airbnb_tools = await airbnb_sess.list_tools()\n",
        "\n",
        "                    functions = (\n",
        "                        [convert_tool(t, \"notes\") for t in local_tools.tools]\n",
        "                        + [convert_tool(t, \"airbnb\") for t in airbnb_tools.tools]\n",
        "                    )\n",
        "\n",
        "                    tool_calls = call_llm(prompt, functions, use_real=USE_REAL_LLM)\n",
        "                    print(\"tool_calls:\", tool_calls)\n",
        "\n",
        "                    tool_results = []\n",
        "                    for call in tool_calls:\n",
        "                        name = call[\"name\"]\n",
        "                        args = call[\"args\"]\n",
        "                        prefix, tool_name = name.split(\"__\", 1)\n",
        "\n",
        "                        if prefix == \"notes\":\n",
        "                            res = await local_sess.call_tool(tool_name, args)\n",
        "                            tool_results.append(\n",
        "                                {\n",
        "                                    \"name\": name,\n",
        "                                    \"args\": args,\n",
        "                                    \"content\": [c.text for c in res.content if hasattr(c, \"text\")],\n",
        "                                }\n",
        "                            )\n",
        "                        elif prefix == \"airbnb\":\n",
        "                            res = await airbnb_sess.call_tool(tool_name, args)\n",
        "                            payload = []\n",
        "                            if hasattr(res, \"content\"):\n",
        "                                for c in res.content:\n",
        "                                    if hasattr(c, \"text\"):\n",
        "                                        payload.append(c.text)\n",
        "                            tool_results.append(\n",
        "                                {\n",
        "                                    \"name\": name,\n",
        "                                    \"args\": args,\n",
        "                                    \"content\": payload,\n",
        "                                }\n",
        "                            )\n",
        "\n",
        "                    # note: DO NOT call answer_with_llm here if you want to inspect things first\n",
        "                    return functions, tool_calls, tool_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70fb035",
      "metadata": {
        "id": "a70fb035"
      },
      "source": [
        "## Demo\n",
        "Adjust the prompt as you like. Switch `USE_REAL_AIRBNB/USE_REAL_LLM` to true when ready."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What tools can you access? List them please.\"\n",
        "\n",
        "# Call orchestrate and display results\n",
        "functions_available, llm_tool_calls, tool_results = await orchestrate(prompt)\n",
        "\n",
        "print(\"=== AVAILABLE TOOLS ===\")\n",
        "for t_dict in functions_available:\n",
        "    fn_info = t_dict['function']\n",
        "    print(f\"- {fn_info['name']}: {fn_info['description']}\")\n",
        "\n",
        "print(\"\\n=== LLM TOOL CALLS ===\")\n",
        "for call in llm_tool_calls:\n",
        "    print(call)\n",
        "\n",
        "\n",
        "print(\"\\n=== TOOL RESULTS ===\")\n",
        "for result in tool_results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD-PfP7bPOSS",
        "outputId": "dec8479b-c385-488a-d63d-4887c56790f3"
      },
      "id": "FD-PfP7bPOSS",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool_calls: []\n",
            "=== AVAILABLE TOOLS ===\n",
            "- notes__add_note: Add a note to the in-memory list.\n",
            "- notes__list_notes: List saved notes.\n",
            "- airbnb__airbnb_search: Search for Airbnb listings with various filters and pagination. Provide direct links to the user\n",
            "- airbnb__airbnb_listing_details: Get detailed information about a specific Airbnb listing. Provide direct links to the user\n",
            "\n",
            "=== LLM TOOL CALLS ===\n",
            "\n",
            "=== TOOL RESULTS ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cdc57a02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc57a02",
        "outputId": "49adef21-28d7-48c4-a807-d00b8a34f0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool_calls: []\n",
            "=== AVAILABLE TOOLS ===\n",
            "- notes__add_note: Add a note to the in-memory list.\n",
            "- notes__list_notes: List saved notes.\n",
            "- airbnb__airbnb_search: Search for Airbnb listings with various filters and pagination. Provide direct links to the user\n",
            "- airbnb__airbnb_listing_details: Get detailed information about a specific Airbnb listing. Provide direct links to the user\n",
            "\n",
            "=== LLM TOOL CALLS ===\n",
            "\n",
            "=== TOOL RESULTS ===\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What tools can you access ? list them please \"\n",
        "\n",
        "functions_available, llm_tool_calls, tool_results = await orchestrate(prompt)\n",
        "\n",
        "print(\"=== AVAILABLE TOOLS ===\")\n",
        "for t_dict in functions_available:\n",
        "    fn_info = t_dict['function']\n",
        "    print(f\"- {fn_info['name']}: {fn_info['description']}\")\n",
        "\n",
        "print(\"\\n=== LLM TOOL CALLS ===\")\n",
        "for call in llm_tool_calls:\n",
        "    print(call)\n",
        "\n",
        "\n",
        "print(\"\\n=== TOOL RESULTS ===\")\n",
        "for result in tool_results:\n",
        "    print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}