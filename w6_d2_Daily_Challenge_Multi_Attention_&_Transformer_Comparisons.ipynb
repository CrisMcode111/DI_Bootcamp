{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj9aVjiwB+xclvNAa7+gFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrisMcode111/DI_Bootcamp/blob/main/w6_d2_Daily_Challenge_Multi_Attention_%26_Transformer_Comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p80sAh0StpYM"
      },
      "outputs": [],
      "source": [
        "\n",
        "You will produce:\n",
        "\n",
        "A custom PyTorch module implementing multi-head attention + feedforward encoder blocks.\n",
        "A fine-tuned transformer baseline on the same dataset.\n",
        "Visualizations of attention weights for selected samples.\n",
        "A reflection comparing both approaches and documenting insights about attention behavior.\n",
        "\n",
        "\n",
        "Dataset\n",
        "Dataset Use the Natural Language Inference dataset provided in this link.\n",
        "\n",
        "Task\n",
        "Single-Head Attention Implementation\n",
        "\n",
        "Objective: Implement the building block before extending to multiple heads.\n",
        "Instructions:\n",
        "Using PyTorch, implement an Attention module with linear projections for Q/K/V."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate shapes with dummy tensors (batch, seq_len, hidden_dim)."
      ],
      "metadata": {
        "id": "SB-nsDDzuqv8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d59509a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        self.q_linear = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        self.k_linear = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        self.v_linear = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Apply linear transformations\n",
        "        q = self.q_linear(query)\n",
        "        k = self.k_linear(key)\n",
        "        v = self.v_linear(value)\n",
        "\n",
        "        # Compute attention scores (scaled dot-product)\n",
        "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "\n",
        "        # Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# Validate shapes with dummy tensors\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "embed_dim = 512\n",
        "head_dim = 64 # embed_dim should be divisible by head_dim in multi-head attention\n",
        "\n",
        "single_head_attention = SingleHeadAttention(embed_dim, head_dim)\n",
        "\n",
        "# Create dummy tensors\n",
        "query = torch.randn(batch_size, seq_len, embed_dim)\n",
        "key = torch.randn(batch_size, seq_len, embed_dim)\n",
        "value = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Forward pass\n",
        "output, attention_weights = single_head_attention(query, key, value)\n",
        "\n",
        "# Print shapes\n",
        "print(\"Query shape:\", query.shape)\n",
        "print(\"Key shape:\", key.shape)\n",
        "print(\"Value shape:\", value.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention weights shape:\", attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log the attention weights for inspection."
      ],
      "metadata": {
        "id": "UMKjWdpYmfOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rapid logging during forward pass (for debugging)\n",
        "\n",
        "# If  just want to see them numerically or save them:\n",
        "output, attn_weights = single_head_attention(query, key, value)\n",
        "\n",
        "print(\"Attention weights shape:\", attn_weights.shape)\n",
        "print(\"Sample attention weights [batch 0, token 0]:\")\n",
        "print(attn_weights[0, 0])\n"
      ],
      "metadata": {
        "id": "6e6awY4wnYfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file (for later analysis)\n",
        "import numpy as np\n",
        "\n",
        "# Salve as file .npy\n",
        "np.save(\"attention_weights.npy\", attn_weights.detach().cpu().numpy())\n",
        "\n",
        "# Or as text (attention)\n",
        "np.savetxt(\"attention_sample.txt\", attn_weights[0, 0].detach().cpu().numpy())\n"
      ],
      "metadata": {
        "id": "Ej1nD7AOng1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WiTqSisPnz5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphical view (the clearest)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example: attention for the first example in the batch\n",
        "attn = attn_weights[0].detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(attn, cmap=\"viridis\")\n",
        "plt.title(\"Attention Weights Heatmap (Sample 0)\")\n",
        "plt.xlabel(\"Keys\")\n",
        "plt.ylabel(\"Queries\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "icHoE5Jans_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attention Module\n",
        "\n",
        "Objective: Extend the single-head block to multi-head functionality.\n",
        "Instructions:\n",
        "Implement MultiHeadAttention that splits embeddings into num_heads, applies attention per head, and concatenates."
      ],
      "metadata": {
        "id": "6Q6YOz0FoyqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention Module (PyTorch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # Linear projections for Q, K, V (shared across all heads)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # Final linear layer to combine heads\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, L, D = x.shape\n",
        "        H, d = self.num_heads, self.head_dim\n",
        "\n",
        "        # 1️⃣ Linear projections\n",
        "        q = self.q_proj(x)  # [B, L, D]\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # 2️⃣ Reshape → separate heads\n",
        "        # becomes [B, H, L, d]\n",
        "        q = q.view(B, L, H, d).transpose(1, 2)\n",
        "        k = k.view(B, L, H, d).transpose(1, 2)\n",
        "        v = v.view(B, L, H, d).transpose(1, 2)\n",
        "\n",
        "        # 3️⃣ Scaled dot-product attention per head\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, L, L]\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)             # attention weights [B, H, L, L]\n",
        "        context = torch.matmul(attn, v)              # weighted values [B, H, L, d]\n",
        "\n",
        "        # 4️⃣ Concatenate heads → [B, L, D]\n",
        "        context = context.transpose(1, 2).contiguous().view(B, L, D)\n",
        "\n",
        "        # 5️⃣ Final linear projection\n",
        "        out = self.out_proj(context)                 # [B, L, D]\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "q6W9_LtGo6Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with dummy tensors\n",
        "# Dummy input\n",
        "batch_size, seq_len, embed_dim, num_heads = 2, 10, 512, 8\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "out, attn_w = mha(x)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", out.shape)           # → [2, 10, 512]\n",
        "print(\"Attention weights shape:\", attn_w.shape)  # → [2, 8, 10, 10]\n"
      ],
      "metadata": {
        "id": "D7Be4439pAKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Include dropout and residual connections.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # Linear projections for Q, K, V and output\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # Regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Save residual for skip connection\n",
        "        residual = x\n",
        "\n",
        "        B, L, D = x.shape\n",
        "        H = self.num_heads\n",
        "        d = self.head_dim\n",
        "\n",
        "        # 1️ Linear projections\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # 2️ Reshape into heads [B, H, L, d]\n",
        "        q = q.view(B, L, H, d).transpose(1, 2)\n",
        "        k = k.view(B, L, H, d).transpose(1, 2)\n",
        "        v = v.view(B, L, H, d).transpose(1, 2)\n",
        "\n",
        "        # 3️ Scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)  # dropout on attention map\n",
        "\n",
        "        context = torch.matmul(attn, v)  # [B, H, L, d]\n",
        "\n",
        "        # 4️ Merge heads\n",
        "        context = context.transpose(1, 2).contiguous().view(B, L, D)\n",
        "        out = self.out_proj(context)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # 5️⃣ Residual + LayerNorm\n",
        "        out = self.norm(out + residual)\n",
        "\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "u7g0N7rIpIzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add FeedForward Network (Positionwise FFN)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.net(x)\n",
        "        return self.norm(x + residual)\n"
      ],
      "metadata": {
        "id": "pVSycWyMpqoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Full Encoder Block (Attention + FFN)\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.ffn = FeedForward(embed_dim, ff_dim, dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x, attn_w = self.mha(x, mask)\n",
        "        x = self.ffn(x)\n",
        "        return x, attn_w\n"
      ],
      "metadata": {
        "id": "u2FSFpPnp63l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to test\n",
        "B, L, D, H = 2, 10, 512, 8\n",
        "x = torch.randn(B, L, D)\n",
        "block = EncoderBlock(embed_dim=D, num_heads=H, ff_dim=2048, dropout=0.1)\n",
        "\n",
        "out, attn_w = block(x)\n",
        "print(\"Output shape:\", out.shape)        # → [2, 10, 512]\n",
        "print(\"Attention weights:\", attn_w.shape)  # → [2, 8, 10, 10]\n"
      ],
      "metadata": {
        "id": "oPjjA-ekqBew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "*  Dropout → regularizes attention and feedforward layers\n",
        "* Residuals → preserve information flow and help gradient stability\n",
        "* LayerNorm → stabilizes activations after each residual connection"
      ],
      "metadata": {
        "id": "-fS1Rh5tqK3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide a forward example showing input/output shapes."
      ],
      "metadata": {
        "id": "JITFxgkDqUqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Dummy batch\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "ff_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "# Create random input (e.g., embedded tokens)\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Initialize encoder block\n",
        "encoder_block = EncoderBlock(embed_dim, num_heads, ff_dim, dropout)\n",
        "\n",
        "# Forward pass\n",
        "output, attn_weights = encoder_block(x)\n",
        "\n",
        "# Print tensor shapes\n",
        "print(f\"Input shape:             {x.shape}\")          # [2, 10, 512]\n",
        "print(f\"Output shape:            {output.shape}\")     # [2, 10, 512]\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\") # [2, 8, 10, 10]\n"
      ],
      "metadata": {
        "id": "Rd3UBfm1qb0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                   | Tensor         | Shape                      | Description                        |\n",
        "| ---------------------- | -------------- | -------------------------- | ---------------------------------- |\n",
        "| Input                  | `x`            | `[B, L, D] = [2, 10, 512]` | Sequence of embeddings             |\n",
        "| Q/K/V projection       | `q, k, v`      | `[2, 8, 10, 64]`           | Split into 8 heads of size 64      |\n",
        "| Attention weights      | `attn_weights` | `[2, 8, 10, 10]`           | Relation between each token pair   |\n",
        "| Weighted sum (context) | `context`      | `[2, 10, 512]`             | Heads concatenated back together   |\n",
        "| FFN output             | `output`       | `[2, 10, 512]`             | Processed representation per token |\n"
      ],
      "metadata": {
        "id": "nho8CEfEqg1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result\n",
        "\n",
        "The encoder block preserves the sequence length (10 → 10) and embedding dimension (512 → 512) while enriching each token with contextual information from all others through multi-head attention."
      ],
      "metadata": {
        "id": "p9eJAezdqlbs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3pCDCDtsf6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXgjJk_RwEGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AQejvG5wti7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6MBpHN_1sh4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsmmrTALsTyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9Ga6NWlrxsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qV_kVefaqdRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcfBGHqkqEcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SpQdyMoYn1BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ND6XlskJmGZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bpPb9YhsmgOc"
      }
    }
  ]
}