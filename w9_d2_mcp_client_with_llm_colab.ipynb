{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrisMcode111/DI_Bootcamp/blob/main/w9_d2_mcp_client_with_llm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f1aa98",
      "metadata": {
        "id": "39f1aa98"
      },
      "source": [
        "# MCP Client with an LLM (Colab)\n",
        "\n",
        "Step-by-step notebook that connects a Python MCP client to a server over STDIO, hands tool schemas to an LLM, and executes the tool calls it suggests."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f0cd08",
      "metadata": {
        "id": "24f0cd08"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "- You need a GitHub Models token in `GITHUB_TOKEN` if you want to actually call the LLM (use Colab Secrets instead of plain text).\n",
        "- Leave `USE_REAL_LLM = False` to dry-run with a simple planner that infers numbers from the prompt.\n",
        "- The notebook creates a minimal MCP server (`add`, `multiply`, and one greeting resource) so everything runs locally over STDIO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21be58e8",
      "metadata": {
        "id": "21be58e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f33ace5-ead1-4548-c9c6-7f3e2b85d5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade mcp azure-ai-inference azure-core nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee186fb0",
      "metadata": {
        "id": "ee186fb0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Optional: set your GitHub Models token for Azure AI Inference.\n",
        "# In Colab, prefer: from google.colab import userdata; os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"GITHUB_TOKEN\")\n",
        "os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"GITHUB_TOKEN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ffa6e40",
      "metadata": {
        "id": "7ffa6e40"
      },
      "source": [
        "## Create a minimal MCP server (STDIO)\n",
        "\n",
        "We use FastMCP to expose two tools and one resource. The `mcp` CLI will launch this file when the client opens a STDIO session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e77246cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e77246cd",
        "outputId": "de4df4ea-d67d-4488-859a-f1fa83eb1a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved MCP server to /content/demo_server.py\n",
            "from mcp.server.fastmcp import FastMCP\n",
            "\n",
            "mcp = FastMCP(\"DemoServer\")\n",
            "\n",
            "@mcp.tool()\n",
            "def add(a: int, b: int) -> int:\n",
            "    'Add two integers.'\n",
            "    return a + b\n",
            "\n",
            "@mcp.tool()\n",
            "def multiply(a: int, b: int) -> int:\n",
            "    'Multiply two integers.'\n",
            "    return a * b\n",
            "\n",
            "@mcp.resource(\"greeting://{name}\")\n",
            "def get_greeting(name: str) -> str:\n",
            "    'Return a greeting string.'\n",
            "    return f\"Hello, {name}!\"\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    mcp.run()\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "SERVER_FILE = Path(\"demo_server.py\")\n",
        "\n",
        "#variable qui recoit valeur\n",
        "SERVER_CODE = '''\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "mcp = FastMCP(\"DemoServer\")\n",
        "\n",
        "@mcp.tool()\n",
        "def add(a: int, b: int) -> int:\n",
        "    'Add two integers.'\n",
        "    return a + b\n",
        "\n",
        "@mcp.tool()\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    'Multiply two integers.'\n",
        "    return a * b\n",
        "\n",
        "@mcp.resource(\"greeting://{name}\")\n",
        "def get_greeting(name: str) -> str:\n",
        "    'Return a greeting string.'\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run()\n",
        "'''\n",
        "\n",
        "SERVER_FILE.write_text(SERVER_CODE.strip() + \"\", encoding=\"utf-8\")\n",
        "print(f\"Saved MCP server to {SERVER_FILE.resolve()}\")\n",
        "print(SERVER_FILE.read_text())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3ZMOMf8XktI",
        "outputId": "39490291-8f7d-491e-be01-fd0da7e821e3"
      },
      "id": "K3ZMOMf8XktI",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765247be",
      "metadata": {
        "id": "765247be"
      },
      "source": [
        "## Client and LLM helpers\n",
        "\n",
        "- Connects to the server over STDIO and lists resources/tools.\n",
        "- Converts MCP tool schemas into an LLM function-calling spec.\n",
        "- Calls the LLM (or a stub) to choose tools, then executes them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a1236b9d",
      "metadata": {
        "id": "a1236b9d"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import nest_asyncio\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "SERVER_FILE = Path(\"demo_server.py\")\n",
        "USE_REAL_LLM = True  # flip to True when GITHUB_TOKEN is set\n",
        "AZURE_ENDPOINT = \"https://models.inference.ai.azure.com\"\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "\n",
        "def convert_to_llm_tool(tool):\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description or \"MCP tool\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": tool.inputSchema.get(\"properties\", {}),\n",
        "                \"required\": tool.inputSchema.get(\"required\", []),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "def stub_tool_calls(prompt: str, functions: List[Dict[str, Any]]):\n",
        "    text = prompt.lower()\n",
        "    numbers = [int(n) for n in re.findall(r\"-?\\d+\", prompt)]\n",
        "    a = numbers[0] if numbers else 2\n",
        "    b = numbers[1] if len(numbers) > 1 else (numbers[0] if numbers else 20)\n",
        "    if any(k in text for k in [\"multiply\", \"product\", \"times\"]):\n",
        "        return [{\"name\": \"multiply\", \"args\": {\"a\": a, \"b\": b}}]\n",
        "    return [{\"name\": \"add\", \"args\": {\"a\": a, \"b\": b}}]\n",
        "\n",
        "def call_llm(prompt: str, functions: List[Dict[str, Any]], use_llm: bool = False):\n",
        "    if not use_llm:\n",
        "        print(\"LLM disabled -> using stubbed plan.\")\n",
        "        return stub_tool_calls(prompt, functions)\n",
        "\n",
        "    token = os.getenv(\"GITHUB_TOKEN\")\n",
        "    if not token:\n",
        "        raise RuntimeError(\"GITHUB_TOKEN is missing. Set it or call with use_llm=False.\")\n",
        "\n",
        "    from azure.ai.inference import ChatCompletionsClient\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=AZURE_ENDPOINT,\n",
        "        credential=AzureKeyCredential(token),\n",
        "    )\n",
        "\n",
        "    response = client.complete(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an MCP planning assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        tools=functions,\n",
        "        temperature=0,\n",
        "        max_tokens=400,\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].message\n",
        "    tool_calls = []\n",
        "    if message.tool_calls:\n",
        "        for call in message.tool_calls:\n",
        "            args = call.function.arguments\n",
        "            args_json = json.loads(args) if isinstance(args, str) else args\n",
        "            tool_calls.append({\"name\": call.function.name, \"args\": args_json})\n",
        "\n",
        "    return tool_calls\n",
        "\n",
        "def simplify_content(content):\n",
        "    rendered = []\n",
        "    for item in content:\n",
        "        if hasattr(item, \"text\"):\n",
        "            rendered.append(item.text)\n",
        "        else:\n",
        "            rendered.append(str(item))\n",
        "    return rendered if len(rendered) > 1 else (rendered[0] if rendered else None)\n",
        "\n",
        "async def orchestrate(prompt: str, use_llm: bool = USE_REAL_LLM):\n",
        "    server_params = StdioServerParameters(\n",
        "        command=\"mcp\",\n",
        "        args=[\"run\", str(SERVER_FILE)],\n",
        "        env=None,\n",
        "    )\n",
        "\n",
        "    # Temporarily redirect sys.stderr for the subprocess to avoid 'UnsupportedOperation: fileno'\n",
        "    temp_stderr_file = None\n",
        "    try:\n",
        "        # Open os.devnull in write mode to get a file object that supports fileno()\n",
        "        temp_stderr_file = open(os.devnull, 'w')\n",
        "\n",
        "        async with stdio_client(server_params, errlog=temp_stderr_file.fileno()) as (read, write):\n",
        "            async with ClientSession(read, write) as session:\n",
        "                print(\"1) Initialize session\")\n",
        "                await session.initialize()\n",
        "\n",
        "                print(\"2) Discover resources and tools\")\n",
        "                resources = await session.list_resources()\n",
        "                tools_response = await session.list_tools()\n",
        "                for tool in tools_response.tools:\n",
        "                    print(f\" - {tool.name}: {tool.inputSchema.get('properties', {})}\")\n",
        "\n",
        "                print(\"3) Convert MCP tools to LLM function spec\")\n",
        "                functions = [convert_to_llm_tool(t) for t in tools_response.tools]\n",
        "\n",
        "                print(\"4) Ask the LLM (or stub) what to call\")\n",
        "                tool_calls = call_llm(prompt, functions, use_llm=use_llm)\n",
        "                print(\"Proposed tool calls:\", tool_calls)\n",
        "\n",
        "                print(\"5) Execute tool calls\")\n",
        "                results = []\n",
        "                for call in tool_calls:\n",
        "                    result = await session.call_tool(call[\"name\"], arguments=call[\"args\"])\n",
        "                    simplified = simplify_content(result.content)\n",
        "                    results.append({\"tool\": call[\"name\"], \"args\": call[\"args\"], \"content\": simplified})\n",
        "                    print(f\" - {call['name']}({call['args']}) -> {simplified}\")\n",
        "\n",
        "                return {\n",
        "                    \"resources\": [str(r) for r in resources],\n",
        "                    \"tools\": [t.name for t in tools_response.tools],\n",
        "                    \"tool_calls\": tool_calls,\n",
        "                    \"results\": results,\n",
        "                }\n",
        "    finally:\n",
        "        # Ensure the temporary file is closed\n",
        "        if temp_stderr_file:\n",
        "            temp_stderr_file.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb140550",
      "metadata": {
        "id": "eb140550"
      },
      "source": [
        "## Run it (addition example)\n",
        "\n",
        "Keep `USE_REAL_LLM=False` to see the flow without calling an external model. Flip it to `True` once `GITHUB_TOKEN` is set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0a95eb28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a95eb28",
        "outputId": "aa270cf7-6d77-4fc8-ce97-13d74b650d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Initialize session\n",
            "2) Discover resources and tools\n",
            " - add: {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
            " - multiply: {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
            "3) Convert MCP tools to LLM function spec\n",
            "4) Ask the LLM (or stub) what to call\n",
            "Proposed tool calls: [{'name': 'add', 'args': {'a': 2, 'b': 20}}]\n",
            "5) Execute tool calls\n",
            " - add({'a': 2, 'b': 20}) -> 22\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'resources': [\"('meta', None)\", \"('nextCursor', None)\", \"('resources', [])\"],\n",
              " 'tools': ['add', 'multiply'],\n",
              " 'tool_calls': [{'name': 'add', 'args': {'a': 2, 'b': 20}}],\n",
              " 'results': [{'tool': 'add', 'args': {'a': 2, 'b': 20}, 'content': '22'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "await orchestrate(\"Add 2 to 20\", use_llm=USE_REAL_LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20289006",
      "metadata": {
        "id": "20289006"
      },
      "source": [
        "## Exercise: multiply 7 by 6\n",
        "\n",
        "Re-run with a different prompt. If you toggle `USE_REAL_LLM=True`, the model should return a `tool_calls` entry for `multiply` with the right arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e8f822a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8f822a7",
        "outputId": "b1555dd1-f031-49e6-ddff-c1d8c51681f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Initialize session\n",
            "2) Discover resources and tools\n",
            " - add: {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
            " - multiply: {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
            "3) Convert MCP tools to LLM function spec\n",
            "4) Ask the LLM (or stub) what to call\n",
            "Proposed tool calls: [{'name': 'multiply', 'args': {'a': 7, 'b': 6}}]\n",
            "5) Execute tool calls\n",
            " - multiply({'a': 7, 'b': 6}) -> 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'resources': [\"('meta', None)\", \"('nextCursor', None)\", \"('resources', [])\"],\n",
              " 'tools': ['add', 'multiply'],\n",
              " 'tool_calls': [{'name': 'multiply', 'args': {'a': 7, 'b': 6}}],\n",
              " 'results': [{'tool': 'multiply', 'args': {'a': 7, 'b': 6}, 'content': '42'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "await orchestrate(\"Multiply 7 by 6\", use_llm=USE_REAL_LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb41f93",
      "metadata": {
        "id": "1eb41f93"
      },
      "source": [
        "## Best practices recap\n",
        "\n",
        "- Keep tool schemas tight and mark required fields to avoid retries.\n",
        "- Only expose the tools you are comfortable running; filter before sending to the LLM.\n",
        "- Log the chain end to end: prompt -> tool_calls -> tool results for debugging and audits.\n",
        "- If validation fails, show the schema and ask the model (or user) to fix arguments.\n",
        "- Use STDIO for local dev; swap to HTTP/SSE for remote hosts without changing tool logic."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}