{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7vKB0j4M/p707UhD9/mzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrisMcode111/DI_Bootcamp/blob/main/w8_d1_evaluating_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you will create\n",
        "You will complete exercises to apply BLEU and ROUGE scores to sample text, analyze perplexity scores, conduct adversarial testing, and propose improvements for LLM evaluation methodologies.\n",
        "\n"
      ],
      "metadata": {
        "id": "d6KwkYzrgHIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Understanding LLM Evaluation:\n",
        "\n",
        "Explain why evaluating LLMs is more complex than traditional software.\n",
        "Identify key reasons for evaluating an LLM’s safety.\n",
        "Describe how adversarial testing contributes to LLM improvement.\n",
        "Discuss the limitations of automated evaluation metrics and how they compare to human evaluation.\n"
      ],
      "metadata": {
        "id": "YG1j3cYwgLS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why LLM Evaluation Is More Complex Than Traditional Software\n",
        "\n",
        "Evaluating an LLM is fundamentally different from evaluating classic deterministic software.\n",
        "\n",
        "Key reasons:\n",
        "\n",
        "Non-deterministic outputs:\n",
        "Traditional software always returns the same result for the same input.\n",
        "LLMs generate probabilistic outputs that can vary each run.\n",
        "\n",
        "Dependence on training data:\n",
        "Models inherit patterns, biases, and errors from massive datasets that developers cannot fully inspect.\n",
        "\n",
        "High sensitivity to prompts:\n",
        "Small changes in wording can dramatically change the output.\n",
        "\n",
        "Emergent behaviors:\n",
        "Large models can display abilities not explicitly programmed.\n",
        "\n",
        "No single “correct” answer:\n",
        "Many tasks (summarization, creative writing, etc.) do not have one definitive ground truth.\n",
        "\n",
        "2. Why We Evaluate LLM Safety\n",
        "\n",
        "LLM safety evaluation is essential because models can unintentionally cause harm.\n",
        "\n",
        "Main motivations:\n",
        "\n",
        "Hallucinations:\n",
        "Models can fabricate facts, references, or instructions.\n",
        "\n",
        "Harmful advice:\n",
        "Guidance involving health, chemicals, self-harm, or dangerous activities can be unsafe.\n",
        "\n",
        "Susceptibility to adversarial prompts:\n",
        "Attackers can try to bypass safety rules (jailbreaks).\n",
        "\n",
        "Bias and discrimination:\n",
        "Models may reproduce or amplify social biases present in training data.\n",
        "\n",
        "Privacy risks:\n",
        "Potential leakage of personal or sensitive information.\n",
        "\n",
        "Malicious use cases:\n",
        "Phishing, misinformation, impersonation, or automated fraud.\n",
        "\n",
        "3. How Adversarial Testing Improves LLMs\n",
        "\n",
        "Adversarial testing challenges the model with intentionally difficult, misleading, or harmful prompts to expose weaknesses.\n",
        "\n",
        "Contributions to model improvement:\n",
        "\n",
        "Identifies vulnerabilities:\n",
        "Reveals where the model breaks, hallucinates, or becomes unsafe.\n",
        "\n",
        "Supports continuous fine-tuning:\n",
        "Each discovered flaw becomes training data for future safety refinements.\n",
        "\n",
        "Captures unexpected behavior:\n",
        "Helps uncover edge cases that normal evaluation would miss.\n",
        "\n",
        "Strengthens red-team processes:\n",
        "Provides structured methods to stress-test safety systems before real-world misuse occurs.\n",
        "\n",
        "4. Limitations of Automated Metrics vs Human Evaluation\n",
        "\n",
        "Automated metrics (BLEU, ROUGE, METEOR, BERTScore, perplexity) provide quick, reproducible measurements — but they do not fully capture language quality.\n",
        "\n",
        "Limitations of automated metrics:\n",
        "\n",
        "BLEU / ROUGE:\n",
        "Focus on n-gram overlap → penalize correct paraphrases or alternative valid expressions.\n",
        "\n",
        "METEOR:\n",
        "Better with synonyms, but still limited in understanding context and intent.\n",
        "\n",
        "Perplexity:\n",
        "Measures how predictable text is to the model — not how good, correct, or useful the text is.\n",
        "\n",
        "BERTScore:\n",
        "Measures semantic similarity, but can miss nuance, tone, humor, or factual accuracy.\n",
        "\n",
        "Why human evaluation matters:\n",
        "\n",
        "Humans can assess qualities that metrics cannot:\n",
        "\n",
        "clarity and coherence\n",
        "\n",
        "fluency and naturalness\n",
        "\n",
        "factual correctness\n",
        "\n",
        "tone and emotional appropriateness\n",
        "\n",
        "usefulness and safety\n",
        "\n",
        "ethical implications\n",
        "\n",
        "user preference\n",
        "\n",
        "Automated metrics are helpful but incomplete. Human judgment remains the gold standard for real-world LLM evaluation."
      ],
      "metadata": {
        "id": "1J4gE-LKhE_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Applying BLEU and ROUGE Metrics:\n",
        "\n",
        "Calculate the BLEU score for the following example:\n",
        "\n",
        "Reference: “Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.”\n",
        "Generated: “Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.”\n",
        "Calculate the ROUGE score for the following example:\n",
        "\n",
        "Reference: “In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.”\n",
        "Generated: “To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.”\n",
        "Provide an analysis of the limitations of BLEU and ROUGE when evaluating creative or context-sensitive text.\n",
        "\n",
        "Suggest improvements or alternative methods for evaluating text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "77Rcp-uUhNX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JefIURXDfxcb",
        "outputId": "3ef60fba-ee6d-403b-a18c-1420223a6d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "BLEU score: 0.06296221772093169\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK if needed\n",
        "!pip install nltk\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "reference = [\"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\".split()]\n",
        "generated = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\".split()\n",
        "\n",
        "chencherry = SmoothingFunction()\n",
        "\n",
        "bleu_score = sentence_bleu(reference, generated, smoothing_function=chencherry.method1)\n",
        "\n",
        "print(\"BLEU score:\", bleu_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "reference = \"In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.\"\n",
        "generated = \"To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.\"\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "scores = scorer.score(reference, generated)\n",
        "\n",
        "print(\"ROUGE-1:\", scores['rouge1'])\n",
        "print(\"ROUGE-2:\", scores['rouge2'])\n",
        "print(\"ROUGE-L:\", scores['rougeL'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N3vHIenh-ap",
        "outputId": "5c22b263-89bd-4b12-a43f-bc356f8c68f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=5fdfd76eec2ff80ec0197beed9c08e311fd2709cbfead21162ec2fabe0db17f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "ROUGE-1: Score(precision=0.47058823529411764, recall=0.3333333333333333, fmeasure=0.39024390243902435)\n",
            "ROUGE-2: Score(precision=0.1875, recall=0.13043478260869565, fmeasure=0.15384615384615383)\n",
            "ROUGE-L: Score(precision=0.35294117647058826, recall=0.25, fmeasure=0.2926829268292683)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations of BLEU and ROUGE for Creative or Context-Sensitive Text\n",
        "\n",
        "BLEU and ROUGE are widely used automatic evaluation metrics, but they were originally designed for structured tasks such as machine translation and summarization, where close overlap with a reference text is desirable.\n",
        "When applied to creative, open-ended, or context-sensitive text, these metrics have significant limitations.\n",
        "\n",
        "1. Overreliance on n-gram overlap\n",
        "\n",
        "Both BLEU and ROUGE compare generated text to a reference by counting shared n-grams.\n",
        "This means:\n",
        "\n",
        "* Valid paraphrases are penalized.\n",
        "\n",
        "* Synonyms, rephrasings, or stylistically different text lead to low scores even if meaning is correct.\n",
        "\n",
        "* Creative writing (narrative, argumentation, explanation) rarely matches word-for-word with a reference.\n",
        "\n",
        "Example:\n",
        "“human oversight is essential” vs. “human supervision is necessary”\n",
        "→ Same meaning, very low n-gram overlap.\n",
        "\n",
        "2. No understanding of semantics or context\n",
        "\n",
        "BLEU/ROUGE measure surface similarity, not meaning.\n",
        "They cannot detect whether:\n",
        "\n",
        "* the answer is factually correct\n",
        "\n",
        "* the reasoning is coherent\n",
        "\n",
        "* the tone is appropriate\n",
        "\n",
        "* the text respects real-world context\n",
        "\n",
        "A sentence can score high even if it contains factual errors, as long as it matches the reference’s wording.\n",
        "\n",
        "3. Penalize diversity and originality\n",
        "\n",
        "For tasks requiring creativity (story generation, open-ended reasoning, dialogue), these metrics discourage innovation.\n",
        "\n",
        "They favor:\n",
        "\n",
        "* rigid similarity\n",
        "\n",
        "* conservative phrasing\n",
        "\n",
        "* repetition of the reference structure\n",
        "\n",
        "But creative tasks often require:\n",
        "\n",
        "* new metaphors\n",
        "\n",
        "* new examples\n",
        "\n",
        "* stylistic variation\n",
        "\n",
        "* reordering of ideas\n",
        "\n",
        "→ BLEU/ROUGE cannot reward this.\n",
        "\n",
        "4. Sensitivity to sentence length\n",
        "\n",
        "Both metrics produce misleading values when:\n",
        "\n",
        "* the generated text is too short (artificially high precision)\n",
        "\n",
        "* the generated text is too long (penalized for content not in reference)\n",
        "\n",
        "This becomes problematic in tasks where elaboration or creativity is expected.\n",
        "\n",
        "5. Lack of sensitivity to coherence and quality\n",
        "\n",
        "BLEU and ROUGE ignore important aspects of language quality:\n",
        "\n",
        "* argument flow\n",
        "\n",
        "* narrative coherence\n",
        "\n",
        "* emotional tone\n",
        "\n",
        "* clarity\n",
        "\n",
        "* persuasion\n",
        "\n",
        "* grammar quality\n",
        "\n",
        "A text can achieve a high BLEU/ROUGE score and still be confusing, poorly structured, or low-quality.\n",
        "\n",
        "6. They assume one “gold standard” reference\n",
        "\n",
        "Creative or context-sensitive tasks do not have a single correct answer.\n",
        "Multiple valid outputs are possible, but BLEU/ROUGE compare only to one reference or a small set.\n",
        "\n",
        "This structurally biases the evaluation against diversity.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "BLEU and ROUGE are useful for structured tasks but insufficient for evaluating creative, context-sensitive, or open-ended LLM outputs.\n",
        "They fail to measure meaning, quality, coherence, nuance, and originality — all essential for real-world language generation.\n",
        "\n",
        "For more reliable assessment, they should be paired with:\n",
        "\n",
        "* human evaluation,\n",
        "\n",
        "* semantic metrics (e.g., BERTScore, Sentence Mover’s Similarity),\n",
        "\n",
        "* task-specific qualitative criteria,\n",
        "\n",
        "* and safety/robustness analysis."
      ],
      "metadata": {
        "id": "8Gmg8G7QiWn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements and Alternative Methods for Evaluating Text Generation\n",
        "\n",
        "Evaluating text generation requires more than surface-level metrics like BLEU and ROUGE. Modern LLMs operate in open-ended, creative, and context-dependent scenarios, so more robust evaluation approaches are needed.\n",
        "\n",
        "Below are key improvements and alternative methodologies.\n",
        "\n",
        "1. Semantic Similarity Metrics\n",
        "\n",
        "Instead of relying on exact word overlap, semantic metrics measure meaning.\n",
        "They compare embeddings rather than surface text.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* BERTScore – uses contextual embeddings to measure semantic similarity\n",
        "\n",
        "* Sentence Mover’s Similarity (SMS) – assesses how well meaning is preserved\n",
        "\n",
        "* Word Mover’s Distance (WMD) – computes the minimal distance between word vectors\n",
        "\n",
        "Advantage:\n",
        "Captures synonymy, paraphrasing, and deeper semantic alignment.\n",
        "\n",
        "2. Human Evaluation (Gold Standard)\n",
        "\n",
        "Human judgment remains the most reliable method for open-ended tasks.\n",
        "\n",
        "Often evaluated on:\n",
        "\n",
        "* fluency and naturalness\n",
        "\n",
        "*  and structure\n",
        "\n",
        "* factual accuracy\n",
        "\n",
        "* helpfulness and relevance\n",
        "\n",
        "* tone and appropriateness\n",
        "\n",
        "* creativity and originality\n",
        "\n",
        "Advantage:\n",
        "Humans can detect nuance, emotion, correctness, ethics, and intent — areas where automated metrics fail.\n",
        "\n",
        "3. Task-Specific Rubrics\n",
        "\n",
        "Different tasks require different evaluation frameworks.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Summarization: faithfulness, completeness, conciseness\n",
        "\n",
        "Dialogue: coherence, empathy, engagement\n",
        "\n",
        "Creative writing: style, originality, voice consistency\n",
        "\n",
        "Instruction following: accuracy, completeness, safety\n",
        "\n",
        "Advantage:\n",
        "Allows precise evaluation aligned with the task goals.\n",
        "\n",
        "4. Model-Based Evaluation (LLM-as-a-Judge)\n",
        "\n",
        "Large models can evaluate other models’ outputs using structured criteria.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "* GPT-4 judging GPT-3 outputs\n",
        "\n",
        "* LLM-raters with chain-of-thought reasoning\n",
        "\n",
        "* Self-consistency scoring\n",
        "\n",
        "Advantages:\n",
        "\n",
        "* Scalable\n",
        "\n",
        "* Correlates surprisingly well with human evaluation\n",
        "\n",
        "Can apply task-specific scoring rubrics\n",
        "\n",
        "Caution:\n",
        "Must be monitored for bias and instability.\n",
        "\n",
        "5. Adversarial Testing and Red-Team Evaluation\n",
        "\n",
        "Tests robustness by challenging the model with difficult, ambiguous, or harmful prompts.\n",
        "\n",
        "Checks for:\n",
        "\n",
        "* safety violations\n",
        "\n",
        "* hallucinations\n",
        "\n",
        "* logical inconsistencies\n",
        "\n",
        "* prompt sensitivity\n",
        "\n",
        "* jailbreak vulnerabilities\n",
        "\n",
        "Advantage:\n",
        "Reveals failure modes that normal evaluation misses.\n",
        "\n",
        "6. Factuality and Groundedness Metrics\n",
        "\n",
        "Useful when models must remain accurate (e.g., news, science, medical).\n",
        "\n",
        "Approaches:\n",
        "\n",
        "* Fact-checking models (FEVER score)\n",
        "\n",
        "* Knowledge-graph alignment\n",
        "\n",
        "* Retrieval consistency tests\n",
        "\n",
        "Advantage:\n",
        "Distinguishes between fluent nonsense and genuinely correct output.\n",
        "\n",
        "7. Diversity and Creativity Metrics\n",
        "\n",
        "For story generation, brainstorming, or open-ended tasks:\n",
        "\n",
        "Metrics:\n",
        "\n",
        "* Distinct-n / Unique-n: counts unique n-grams\n",
        "\n",
        "* Entropy / lexical richness: measures vocabulary diversity\n",
        "\n",
        "* Novelty metrics: detect originality relative to training data\n",
        "\n",
        "Advantage:\n",
        "Rewards creativity instead of punishing it.\n",
        "\n",
        "8. Holistic Evaluation Frameworks\n",
        "\n",
        "Combining multiple evaluation types results in a more accurate overall assessment.\n",
        "\n",
        "Examples:\n",
        "\n",
        "HELM (Holistic Evaluation of Language Models)\n",
        "\n",
        "G-Eval (LLM-based rubric evaluation)\n",
        "\n",
        "MMLU, TruthfulQA, SafetyBench\n",
        "\n",
        "Advantage:\n",
        "Captures performance, safety, fairness, robustness, and real-world reliability.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Evaluating text generation requires moving beyond simple n-gram metrics.\n",
        "A strong evaluation pipeline should combine:\n",
        "\n",
        "* semantic metrics (BERTScore, SMS)\n",
        "\n",
        "* human rating\n",
        "\n",
        "* task-specific criteria\n",
        "\n",
        "* safety & robustness tests\n",
        "\n",
        "* LLM-as-a-judge frameworks\n",
        "\n",
        "This multi-layered approach provides a far more accurate, fair, and meaningful assessment of modern text-generation models."
      ],
      "metadata": {
        "id": "4KuBPGKljNOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Perplexity Analysis:\n",
        "\n",
        "Compare the perplexity of the two language models based on the probability assigned to a word:\n",
        "\n",
        "Model A: Assigns 0.8 probability to “mitigation.”\n",
        "Model B: Assigns 0.4 probability to “mitigation.”\n",
        "Determine which model has lower perplexity and explain why.\n",
        "\n",
        "Given a language model that has a perplexity score of 100, discuss its performance implications and possible ways to improve it."
      ],
      "metadata": {
        "id": "g1e87cS4j3Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Perplexity Analysis\n",
        "\n",
        "## Model Comparison Based on Probability\n",
        "\n",
        "Perplexity (PP) for a single word is defined as:\n",
        "\n",
        "$$\n",
        "PP = \\frac{1}{P(w)}\n",
        "$$\n",
        "\n",
        "Where \\( P(w) \\) is the probability the model assigns to the word.\n",
        "\n",
        "### Given:\n",
        "- Model A: \\( P = 0.8 \\)\n",
        "- Model B: \\( P = 0.4 \\)\n",
        "\n",
        "### Perplexity Calculation:\n",
        "\n",
        "$$\n",
        "PP_A = \\frac{1}{0.8} = 1.25\n",
        "$$\n",
        "\n",
        "$$\n",
        "PP_B = \\frac{1}{0.4} = 2.5\n",
        "$$\n",
        "\n",
        "### Conclusion\n",
        "Model **A** has lower perplexity (1.25), meaning it is **more confident** and performs better on the word **\"mitigation\"**.\n",
        "\n",
        "---\n",
        "\n",
        "## Interpreting a Perplexity Score of 100\n",
        "\n",
        "A perplexity of **100** indicates the model is highly uncertain about predictions — almost as if it must choose among **100 equally probable next words**.\n",
        "\n",
        "### Implications:\n",
        "- The model is not predicting well.\n",
        "- It may be poorly trained or poorly matched to the domain.\n",
        "- It shows weak confidence in its probability distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## Ways to Improve a High Perplexity Score\n",
        "\n",
        "- Increase the size of the training dataset.\n",
        "- Improve data cleaning and consistency.\n",
        "- Use a larger or more advanced model architecture.\n",
        "- Fine-tune on domain-specific text.\n",
        "- Optimize hyperparameters (learning rate, dropout, etc.).\n",
        "- Use better tokenization adapted to the domain.\n",
        "\n"
      ],
      "metadata": {
        "id": "hIAinPQ7kvRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Human Evaluation Exercise:\n",
        "\n",
        "Rate the fluency of this chatbot response using a Likert scale (1-5): “Apologies, but comprehend I do not. Could you rephrase your question?”\n",
        "Justify your rating.\n",
        "Propose an improved version of the response and explain why it is better."
      ],
      "metadata": {
        "id": "VoISgAR8lCiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Human Evaluation Exercise\n",
        "\n",
        "## Fluency Rating (Likert Scale 1–5)\n",
        "\n",
        "**Rating: 2 / 5**\n",
        "\n",
        "### Justification\n",
        "The response is grammatically unusual and unnatural in English.  \n",
        "The phrase **“comprehend I do not”** resembles Yoda-style speech, which reduces clarity and fluency.  \n",
        "While the intention is understandable, the structure is not typical for natural conversational English.\n",
        "\n",
        "---\n",
        "\n",
        "## Improved Version\n",
        "\n",
        "**Improved response:**  \n",
        "“I'm sorry, I didn't quite understand that. Could you please rephrase your question?”\n",
        "\n",
        "### Why this version is better:\n",
        "- **Natural and fluent** phrasing  \n",
        "- **Clear and polite tone**  \n",
        "- **Uses standard English grammar**  \n",
        "- **Keeps the meaning identical**  \n",
        "- **Improves user experience** by sounding more helpful and less robotic\n",
        "\n"
      ],
      "metadata": {
        "id": "sJ8-7gIPk_hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Adversarial Testing Exercise:\n",
        "\n",
        "Identify the potential mistake an LLM might make when answering the Prompt: “What is the capitol of France?”\n",
        "\n",
        "Expected: “Paris.”\n",
        "Suggest a method to improve robustness against such errors.\n",
        "\n",
        "Create at least three tricky prompts that could challenge an LLM’s robustness, bias detection, or factual accuracy."
      ],
      "metadata": {
        "id": "XeqA_AX5lMjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Adversarial Testing Exercise\n",
        "\n",
        "## Potential Mistake for the Prompt:\n",
        "**Prompt:** “What is the *capitol* of France?”\n",
        "\n",
        "### Possible LLM Mistake\n",
        "The model may interpret **“capitol”** literally as:\n",
        "- a government **building** (like “the Capitol” in Washington, D.C.)\n",
        "- or may attempt to describe *a type of structure*, rather than understanding the user meant **“capital”**.\n",
        "\n",
        "This can lead to wrong or confused outputs such as:\n",
        "- “France does not have a Capitol building like the U.S.”\n",
        "- “The French capitol is located in Paris but functions differently…”\n",
        "\n",
        "### Expected Correct Answer\n",
        "**Paris.**\n",
        "\n",
        "---\n",
        "\n",
        "## Method to Improve Robustness\n",
        "A robust approach includes:\n",
        "\n",
        "### **Spell-checking + Semantic Intent Inference**\n",
        "The model should:\n",
        "1. Detect likely misspellings (“capitol” → “capital”).  \n",
        "2. Infer user intent based on context.  \n",
        "3. Confirm or cl\n"
      ],
      "metadata": {
        "id": "E4YXQC0LlUAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Comparative Analysis of Evaluation Methods:\n",
        "\n",
        "Choose an NLP task (e.g., machine translation, text summarization, question answering).\n",
        "Compare and contrast at least three different evaluation metrics (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation, etc.).\n",
        "Discuss which metric is most appropriate for the chosen task and why.\n"
      ],
      "metadata": {
        "id": "-JggBj9DlcC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comparative Analysis of Evaluation Methods\n",
        "\n",
        "## Chosen NLP Task: **Text Summarization**\n",
        "\n",
        "Text summarization requires both **semantic accuracy** (capturing the core meaning) and **linguistic quality** (fluency, coherence).  \n",
        "Below is a comparison of commonly used evaluation methods for this task.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **ROUGE**\n",
        "\n",
        "### What it measures:\n",
        "- n-gram overlap between the generated summary and a reference summary  \n",
        "- ROUGE-1 (unigram overlap)  \n",
        "- ROUGE-2 (bigram overlap)  \n",
        "- ROUGE-L (longest common subsequence)\n",
        "\n",
        "### Strengths:\n",
        "- Standard metric widely used in summarization research  \n",
        "- Easy to compute and compare  \n",
        "- Captures whether key words and phrases match the reference\n",
        "\n",
        "### Weaknesses:\n",
        "- Rewards surface similarity, not meaning  \n",
        "- Penalizes valid paraphrases  \n",
        "- Does not detect factual errors  \n",
        "- Does not measure coherence or fluency\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **BERTScore**\n",
        "\n",
        "### What it measures:\n",
        "- Semantic similarity using contextual embeddings from transformer models  \n",
        "- Evaluates meaning rather than exact wording\n",
        "\n",
        "### Strengths:\n",
        "- Captures synonyms and paraphrasing  \n",
        "- Better correlation with human judgments than ROUGE  \n",
        "- Sensitive to deeper semantic alignment\n",
        "\n",
        "### Weaknesses:\n",
        "- More computationally expensive  \n",
        "- May still miss factual inconsistencies  \n",
        "- Performance depends on the underlying language model\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Human Evaluation**\n",
        "\n",
        "### What it measures:\n",
        "Humans rate the summary on:\n",
        "- Fluency  \n",
        "- Coherence  \n",
        "- Faithfulness (no hallucinations)  \n",
        "- Coverage of key information  \n",
        "- Usefulness and readability\n",
        "\n",
        "### Strengths:\n",
        "- Best method for open-ended and creative tasks  \n",
        "- Can detect nuance, tone, and factuality  \n",
        "- Not fooled by paraphrasing or reordering of information\n",
        "\n",
        "### Weaknesses:\n",
        "- Time-consuming  \n",
        "- Expensive  \n",
        "- Not easily scalable  \n",
        "- Subjective unless guided by a scoring rubric\n",
        "\n",
        "---\n",
        "\n",
        "## 4. **Comparison for Text Summarization**\n",
        "\n",
        "| Metric        | Captures Meaning | Sensitive to Paraphrasing | Detects Factual Errors | Measures Fluency | Scalable |\n",
        "|---------------|------------------|---------------------------|-------------------------|------------------|----------|\n",
        "| **ROUGE**     | ❌ Partial        | ❌ No                     | ❌ No                   | ❌ No            | ✔ Yes    |\n",
        "| **BERTScore** | ✔ Good           | ✔ Yes                    | ❌ No                   | ❌ Limited       | ✔ Medium |\n",
        "| **Human Eval**| ✔ Excellent      | ✔ Excellent              | ✔ Yes                  | ✔ Yes           | ❌ No     |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Most Appropriate Metric for Text Summarization**\n",
        "\n",
        "### **Best single metric: BERTScore**  \n",
        "Because:\n",
        "- it evaluates **semantic similarity**,  \n",
        "- allows freedom in wording,  \n",
        "- aligns more closely with how humans judge summary quality.\n",
        "\n",
        "### **But the ideal evaluation combines:**\n",
        "- **ROUGE** to compare key content overlap,  \n",
        "- **BERTScore** to assess semantic preservation,  \n",
        "- **Human evaluation** to evaluate fluency, coherence, and factual consistency.\n",
        "\n",
        "### Final Conclusion:\n",
        "**No single metric is sufficient.**  \n",
        "For text summarization, a hybrid approach provides the most r\n"
      ],
      "metadata": {
        "id": "JGXCrtWOloZB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBEH5pTslMNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "crCtrtyKgBH_"
      }
    }
  ]
}